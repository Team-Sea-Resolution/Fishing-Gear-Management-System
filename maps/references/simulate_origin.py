import math
import os
from urllib.parse import urlencode
#â€“â€“ PROJ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (pyproj â‰¥3ìš© PROJ_DATA í¬í•¨)
os.environ['PROJ_LIB']  = r'C:\Users\HUFS\anaconda3\envs\opendrift_env\Library\share\proj'
os.environ['PROJ_DATA'] = r'C:\Users\HUFS\anaconda3\envs\opendrift_env\Library\share\proj'

import json
import glob
import time
from datetime import datetime, timedelta


import csv
from dateutil import parser
import numpy as np
import pandas as pd
import xarray as xr
import cdsapi
import requests
import geopandas as gpd
from geopy.distance import geodesic
from shapely.geometry import Point, LineString

import matplotlib.pyplot as plt
from scipy.interpolate import griddata

from opendrift.models.oceandrift import OceanDrift
from opendrift.readers import reader_netCDF_CF_generic
from collections import OrderedDict

import matplotlib.pyplot as plt
import matplotlib as mpl
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

plt.rcParams['font.family'] = 'Malgun Gothic'
mpl.rcParams['axes.unicode_minus'] = False



geojson_dir = r"D:\ì–´ì„ í–‰ì ë°ì´í„°\Training\02.ë¼ë²¨ë§ë°ì´í„°\TL_01.ìë§.zip (2)"
# geojson_dir = r'D:\ì–´ì„ í–‰ì ë°ì´í„°\Validation\02.ë¼ë²¨ë§ë°ì´í„°\VL_01.ìë§.zip'
geojson_files = glob.glob(os.path.join(geojson_dir, "*.geojson"))



error_log_path = os.path.join(geojson_dir, "error_log.csv")

# ì—ëŸ¬ ë¡œê·¸ ì´ˆê¸°í™”
if not os.path.exists(error_log_path):
    with open(error_log_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["íŒŒì¼ëª…", "ì˜¤ë¥˜ì¢…ë¥˜", "ì˜¤ë¥˜ë©”ì‹œì§€"])

def get_retry_session(max_retries=5, backoff_factor=1):
    """ì¼ì‹œì ì¸ ì„œë²„/ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ì— ëŒ€í•´ ì¬ì‹œë„ ì •ì±…ì„ ê°€ì§„ requests.Session ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    # ì¬ì‹œë„í•  HTTP ìƒíƒœ ì½”ë“œì™€ GET ë©”ì„œë“œ ì„¤ì •
    retry_strategy = Retry(
        total=max_retries, 
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS"],
        backoff_factor=backoff_factor # 1ì´ˆ, 2ì´ˆ, 4ì´ˆ, ... ê°„ê²©ìœ¼ë¡œ ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    http = requests.Session()
    http.mount("http://", adapter)
    http.mount("https://", adapter)
    return http

# 2. ì „ì—­ ì„¸ì…˜ ë° íƒ€ì„ì•„ì›ƒ ê°’ ì„¤ì •
HTTP_SESSION = get_retry_session()
# ì—°ê²°(Connect) 10ì´ˆ, ë°ì´í„° ìˆ˜ì‹ (Read) 60ì´ˆ ì„¤ì • (ì´ 70ì´ˆ)
TIMEOUT_TUPLE = (10, 60) 

# ì „ì²´ ìë™ ì²˜ë¦¬ ë£¨í”„
for input_file in geojson_files:
    # try:
        input_filename = os.path.basename(input_file)
        print(input_filename)
        if input_filename not in ['T01_DDJ134DGJ_2021-12-08 113000-114.geojson', 'T01_DDJ142HDJ_2021-06-22 150100-019.geojson', 'T01_DDJ134DGJ_2021-09-01 221500-019.geojson']:
            continue

        visibility = None            # ê°€ì‹œê±°ë¦¬(m)
        distance_km = None           # ì¤‘ê°„ íˆ¬ë§ â†” ì–‘ë§ ê±°ë¦¬(km)
        prediction_result = "íŒë‹¨ ë¶ˆê°€"  # ì˜ˆì¸¡ ì„±ê³µ ì—¬ë¶€

        print(f"\n===== ì²˜ë¦¬ ì‹œì‘: {input_filename} =====")

        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        TARGET_SEQ = [3,0,3,1,3]   # ì‹œí€€ìŠ¤ íƒìƒ‰ìš©

        ###############################################################################
        # PART 1: GeoJSON â†’ DataFrame (íˆ¬ë§ ê¶¤ì  ì¶”ì¶œ)
        ###############################################################################
        rows = []
        for feat in data.get("features", []):
            p = feat["properties"]
            beh = p.get("fishery_behavior")
            rows.append({
                "time_stamp": p["time_stamp"],
                "lon":         p["longitude"],
                "lat":         p["latitude"],
                "fishery_behavior": beh
            })

        df = pd.DataFrame(rows)
        if df.empty:
            print(f"âš ï¸ {input_filename}: DataFrame ë¹„ì–´ìˆìŒ â†’ ê±´ë„ˆëœ€")
            continue

        print(f"ğŸ“Š {input_filename}: DataFrame í¬ê¸° = {df.shape}")

        df['time_stamp'] = pd.to_datetime(df['time_stamp'], errors='coerce')
        df = df.sort_values('time_stamp').reset_index(drop=True)

        # ğŸ“Œ ì‹œí€€ìŠ¤ íƒìƒ‰ í•¨ìˆ˜
        def has_target_sequence(behaviors, target=TARGET_SEQ):
            if len(behaviors) == 0:
                print("âš ï¸ behaviors ë¹„ì–´ìˆìŒ")
                return False
            if len(behaviors) < len(target):
                print(f"âš ï¸ behaviors ê¸¸ì´({len(behaviors)}) < target ê¸¸ì´({len(target)})")
                return False

            print(f"â–¶ raw behaviors ê¸¸ì´={len(behaviors)} â†’ {behaviors[:20]}...")  # ì•ë¶€ë¶„ë§Œ í™•ì¸
            compressed = [behaviors[0]]
            for b in behaviors[1:]:
                if b != compressed[-1]:
                    compressed.append(b)

            print(f"â–¶ compressed behaviors ê¸¸ì´={len(compressed)} â†’ {compressed}")

            # TARGET_SEQ í¬í•¨ ì—¬ë¶€ í™•ì¸
            n, m = len(compressed), len(target)
            for i in range(n - m + 1):
                if compressed[i:i+m] == target:
                    print(f"âœ… TARGET_SEQ {target} ë°œê²¬ ìœ„ì¹˜: {i}")
                    return True
            return False

        # ğŸ“Œ ì‹œí€€ìŠ¤ ì—†ëŠ” íŒŒì¼ì€ ìŠ¤í‚µ
        if not has_target_sequence(df['fishery_behavior'].tolist(), TARGET_SEQ):
            print(f"âŒ {input_filename}: ì‹œí€€ìŠ¤ {TARGET_SEQ} ì—†ìŒ â†’ ê±´ë„ˆëœ€")
            continue

        # íˆ¬ë§ ì‹œì‘ ì§€ì ë§Œ í•„í„°ë§ (1->3 ë˜ëŠ” 0->3 ë³€í™” ì‹œì )
        df['prev_behavior'] = df['fishery_behavior'].shift(1)
        drop_points = df[
            (df['fishery_behavior'] == 3) &
            (df['prev_behavior'] != 3)
        ].copy()

        print(f"ğŸ¯ drop_points ê°œìˆ˜: {len(drop_points)}")

        # ğŸ“Œ 2. ì‹œê°„ ë¦¬ìŠ¤íŠ¸ (1ì‹œê°„ ê°„ê²©)
        start_time = df['time_stamp'].min().replace(minute=0, second=0)
        end_time = df['time_stamp'].max()
        print(f"ğŸ•’ start_time={start_time}, end_time={end_time}")

        time_list = []
        current_time = start_time
        while current_time <= end_time:
            time_list.append(current_time)
            current_time += timedelta(hours=1)

        print(f"ğŸ•’ time_list ê¸¸ì´: {len(time_list)} â†’ {time_list[:5]}...")

        simulation_duration = end_time - start_time
        print(f"â³ simulation_duration: {simulation_duration}")

        # ì—°, ì›”, ì¼ ë¬¸ìì—´ë¡œ ì¶”ì¶œ
        year  = f"{start_time.strftime('%Y')}"
        month = f"{start_time.strftime('%m')}"
        day   = f"{start_time.strftime('%d')}"
        print(f"ğŸ“… ë‚ ì§œ: {year}-{month}-{day}")

        lat_min = df['lat'].min() - 0.1
        lat_max = df['lat'].max() + 0.1
        lon_min = df['lon'].min() - 0.1
        lon_max = df['lon'].max() + 0.1

        print(f"ğŸŒ lat ë²”ìœ„=({lat_min}, {lat_max}), lon ë²”ìœ„=({lon_min}, {lon_max})")

        lat_grid = np.arange(round(lat_min, 2), round(lat_max, 2) + 0.01, 0.01)
        lon_grid = np.arange(round(lon_min, 2), round(lon_max, 2) + 0.01, 0.01)

        print(f"ğŸŒ lat_grid={lat_grid.shape}, lon_grid={lon_grid.shape}")
        print(f"===== ì²˜ë¦¬ ì™„ë£Œ: {input_filename} =====\n")



        # ====== NetCDF íŒŒì¼ ê²½ë¡œ ë¯¸ë¦¬ ì„¤ì • ======
        input_basename = os.path.splitext(os.path.basename(input_file))[0]
        nc_folder = r"C:\Users\HUFS\Desktop\opendrift_middle\KHOA_nc_data"
        os.makedirs(nc_folder, exist_ok=True)


            # ====== API í˜¸ì¶œ ë° ë³´ê°„ ìˆ˜í–‰ ======
        service_key = 'ANM8LV6zTsRNiGg6FCUMpw=='
        base_url = "http://www.khoa.go.kr/api/oceangrid/tidalCurrentAreaGeoJson/search.do"
        all_data = []


        output_path = os.path.join(nc_folder, f"{input_basename}_uv.nc")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 2. ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œ ì„ íƒ í•¨ìˆ˜
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        def get_sorted_stations(station_df, lat, lon):
            station_df = station_df.copy()
            station_df['distance'] = station_df.apply(
                lambda row: geodesic((row['lat'], row['lon']), (lat, lon)).km, axis=1
            )
            return station_df.sort_values('distance')

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # 3. KHOA í•´ë¥˜ API í˜¸ì¶œ ë° NetCDF ì €ì¥
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        all_data = []
        base_url = "http://www.khoa.go.kr/api/oceangrid/tidalCurrentAreaGeoJson/search.do"
        for t in time_list:
            params = {
                "DataType": "tidalCurrentAreaGeoJson",
                "ServiceKey": service_key,
                "Date": t.strftime("%Y%m%d"),
                "Hour": t.strftime("%H"),
                "Minute": "00",
                "MinX": lon_min, "MaxX": lon_max,
                "MinY": lat_min, "MaxY": lat_max,
                "Scale": 1000000
            }
            resp = requests.get(base_url, params=params)
            if resp.status_code != 200 or not resp.text.startswith('{'):
                print(f"âŒ API ì‹¤íŒ¨({resp.status_code}) at {t}")
                continue

            for feat in resp.json().get('features', []):
                p = feat['properties']
                lat, lon = p.get('lat'), p.get('lon')
                spd_raw, direction = p.get('current_speed'), p.get('current_direct')
                if None in (lat, lon, spd_raw, direction):
                    continue
                spd = spd_raw / 100.0
                radian = math.radians(direction)
                u = spd * math.sin(radian)   # ë™ìª½ ì„±ë¶„ (xì¶•)
                v = spd * math.cos(radian)   # ë¶ìª½ ì„±ë¶„ (yì¶•)
                all_data.append({
                    'time': t,
                    'lat': lat,
                    'lon': lon,
                    'u': u,
                    'v': v
                })
        df_all = pd.DataFrame(all_data)
        df_all['time'] = pd.to_datetime(df_all['time']).dt.tz_localize(None)
        times = np.array(sorted(df_all['time'].unique()), dtype='datetime64[ns]')
        lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)
        u_interp, v_interp = [], []
        for t in times:
            t = pd.Timestamp(t).replace(tzinfo=None) 
            sub = df_all[df_all['time'] == t]
            pts = sub[['lon', 'lat']].values
            u_grid = griddata(pts, sub['u'], (lon_mesh, lat_mesh), method='linear')
            v_grid = griddata(pts, sub['v'], (lon_mesh, lat_mesh), method='linear')
            u_interp.append(u_grid)
            v_interp.append(v_grid)

        uv_ds = xr.Dataset(
            {
                'x_sea_water_velocity': (['time', 'lat', 'lon'], np.array(u_interp)),
                'y_sea_water_velocity': (['time', 'lat', 'lon'], np.array(v_interp))
            },
            coords={
                'time': times,
                'lat': lat_grid,
                'lon': lon_grid
            },  
            attrs={
                'title': "KHOA í•´ë¥˜ ì˜ˆì¸¡ ë°ì´í„° (ub/vb ë³µì • ì ìš©)",
                'source': "tidalCurrentAreaGeoJson API"
            }
        )
        uv_ds['x_sea_water_velocity'].attrs.update(standard_name="x_sea_water_velocity", units="m s-1")
        uv_ds['y_sea_water_velocity'].attrs.update(standard_name="y_sea_water_velocity", units="m s-1")


        # ì¡°ìœ„ê´€ì¸¡ì†Œ ë°ì´í„°
        tide_data = [
            ["DT_0063", "ê°€ë•ë„", 35.024, 128.81],
            ["DT_0031", "ê±°ë¬¸ë„", 34.028, 127.308],
            ["DT_0029", "ê±°ì œë„", 34.801, 128.699],
            ["DT_0026", "ê³ í¥ë°œí¬", 34.481, 127.342],
            ["DT_0018", "êµ°ì‚°", 35.975, 126.563],
            ["DT_0017", "ëŒ€ì‚°", 37.007, 126.352],
            ["DT_0062", "ë§ˆì‚°", 35.197, 128.576],
            ["DT_0023", "ëª¨ìŠ¬í¬", 33.214, 126.251],
            ["DT_0007", "ëª©í¬", 34.779, 126.375],
            ["DT_0006", "ë¬µí˜¸", 37.55, 129.116],
            ["DT_0025", "ë³´ë ¹", 36.406, 126.486],
            ["DT_0005", "ë¶€ì‚°", 35.096, 129.035],
            ["DT_0061", "ì‚¼ì²œí¬", 34.924, 128.069],
            ["DT_0094", "ì„œê±°ì°¨ë„", 34.251, 125.915],
            ["DT_0010", "ì„œê·€í¬", 33.24, 126.561],
            ["DT_0022", "ì„±ì‚°í¬", 33.474, 126.927],
            ["DT_0012", "ì†ì´ˆ", 38.207, 128.594],
            ["IE_0061", "ì‹ ì•ˆê°€ê±°ì´ˆ", 33.941, 124.592],
            ["DT_0008", "ì•ˆì‚°", 37.192, 126.647],
            ["DT_0067", "ì•ˆí¥", 36.674, 126.129],
            ["DT_0037", "ì–´ì²­ë„", 36.117, 125.984],
            ["DT_0016", "ì—¬ìˆ˜", 34.747, 127.765],
            ["IE_0062", "ì˜¹ì§„ì†Œì²­ì´ˆ", 37.423, 124.738],
            ["DT_0027", "ì™„ë„", 34.315, 126.759],
            ["DT_0013", "ìš¸ë¦‰ë„", 37.491, 130.913],
            ["DT_0020", "ìš¸ì‚°", 35.501, 129.387],
            ["IE_0060", "ì´ì–´ë„", 32.122, 125.182],
            ["DT_0001", "ì¸ì²œ", 37.451, 126.592],
            ["DT_0004", "ì œì£¼", 33.527, 126.543],
            ["DT_0028", "ì§„ë„", 34.377, 126.308],
            ["DT_0021", "ì¶”ìë„", 33.961, 126.3],
            ["DT_0014", "í†µì˜", 34.827, 128.434],
            ["DT_0002", "í‰íƒ", 36.966, 126.822],
            ["DT_0091", "í¬í•­", 36.051, 129.376],
            ["DT_0011", "í›„í¬", 36.677, 129.453],
            ["DT_0035", "í‘ì‚°ë„", 34.684, 125.435],
        ]

        df_tide = pd.DataFrame(tide_data, columns=["obs_code", "name", "lat", "lon"])


        # í•´ìƒê´€ì¸¡ë¶€ì´ ë°ì´í„°
        buoy_data = [
            ["TW_0088", "ê°ì²œí•­", 35.052, 129.003],
            ["TW_0077", "ê²½ì¸í•­", 37.523, 126.592],
            ["TW_0089", "ê²½í¬ëŒ€í•´ìˆ˜ìš•ì¥", 37.808, 128.931],
            ["TW_0095", "ê³ ë˜ë¶ˆí•´ìˆ˜ìš•ì¥", 36.58, 129.454],
            ["TW_0074", "ê´‘ì–‘í•­", 34.859, 127.792],
            ["TW_0072", "êµ°ì‚°í•­", 35.984, 126.508],
            ["TW_0091", "ë‚™ì‚°í•´ìˆ˜ìš•ì¥", 38.122, 128.65],
            ["KG_0025", "ë‚¨í•´ë™ë¶€", 34.222, 128.419],
            ["TW_0069", "ëŒ€ì²œí•´ìˆ˜ìš•ì¥", 36.274, 126.457],
            ["TW_0085", "ë§ˆì‚°í•­", 35.103, 128.631],
            ["TW_0094", "ë§ìƒí•´ìˆ˜ìš•ì¥", 37.616, 129.103],
            ["TW_0086", "ë¶€ì‚°í•­ì‹ í•­", 35.043, 128.761],
            ["TW_0079", "ìƒì™•ë“±ë„", 35.652, 126.194],
            ["TW_0081", "ìƒì¼ë„", 34.258, 126.96],
            ["TW_0093", "ì†ì´ˆí•´ìˆ˜ìš•ì¥", 38.198, 128.631],
            ["TW_0083", "ì—¬ìˆ˜í•­", 34.794, 127.808],
            ["TW_0078", "ì™„ë„í•­", 34.325, 126.763],
            ["TW_0080", "ìš°ì´ë„", 34.543, 125.802],
            ["KG_0101", "ìš¸ë¦‰ë„ë¶ë™", 38.007, 131.552],
            ["KG_0102", "ìš¸ë¦‰ë„ë¶ì„œ", 37.742, 130.601],
            ["TW_0076", "ì¸ì²œí•­", 37.389, 126.533],
            ["KG_0021", "ì œì£¼ë‚¨ë¶€", 32.09, 126.965],
            ["KG_0028", "ì œì£¼í•´í˜‘", 33.7, 126.59],
            ["TW_0075", "ì¤‘ë¬¸í•´ìˆ˜ìš•ì¥", 33.234, 126.409],
            ["TW_0082", "íƒœì•ˆí•­", 37.006, 126.27],
            ["TW_0084", "í†µì˜í•­", 34.773, 128.46],
            ["TW_0070", "í‰íƒë‹¹ì§„í•­", 37.136, 126.54],
            ["HB_0002", "í•œìˆ˜ì›_ê³ ë¦¬", 35.318, 129.314],
            ["HB_0001", "í•œìˆ˜ì›_ê¸°ì¥", 35.182, 129.235],
            ["HB_0009", "í•œìˆ˜ì›_ë‚˜ê³¡", 37.119, 129.395],
            ["HB_0008", "í•œìˆ˜ì›_ë•ì²œ", 37.1, 129.404],
            ["HB_0007", "í•œìˆ˜ì›_ì˜¨ì–‘", 37.019, 129.425],
            ["HB_0003", "í•œìˆ˜ì›_ì§„í•˜", 35.384, 129.368],
        ]

        df_buoy = pd.DataFrame(buoy_data, columns=["obs_code", "name", "lat", "lon"])

        total_stations = pd.concat([df_tide, df_buoy], ignore_index=True)
        sorted_stations = get_sorted_stations(total_stations, lat, lon)

        temp_records = []
        for _, row in sorted_stations.iterrows():
            obs_code = row['obs_code']
            data_type = "tideObsTemp" if obs_code.startswith("DT") or obs_code.startswith("IE") else "tidalBuTemp"
            url_with_key = f"http://www.khoa.go.kr/api/oceangrid/{data_type}/search.do?ServiceKey={service_key}"

            temp_records.clear()
            for date_str in sorted(set(t.strftime('%Y%m%d') for t in time_list)):
                params = {
                    "ObsCode": obs_code,
                    "Date": date_str,
                    "ResultType": "json"
                }
                r = requests.get(url_with_key, params=params)
                if not r.ok:
                    continue
                result_json = r.json()
                if result_json.get("result", {}).get("error") == "No search data":
                    continue

                for rec in result_json.get("result", {}).get("data", []):
                    try:
                        temp_records.append({
                            "time": pd.to_datetime(rec["record_time"]),
                            "sea_water_temperature": float(rec["water_temp"])
                        })
                    except:
                        continue

            if temp_records:
                print(f"âœ… ìˆ˜ì˜¨ ë°ì´í„° ì‚¬ìš© ê´€ì¸¡ì†Œ: {row['name']} ({obs_code})")
                break  # âœ”ï¸ í•œ ê´€ì¸¡ì†Œì—ì„œ ë°ì´í„° ìˆ˜ì§‘ë˜ë©´ ì¢…ë£Œ

        if not temp_records:
            print("âš ï¸ ìˆ˜ì˜¨ ë°ì´í„° ì—†ìŒ (ëª¨ë“  ê´€ì¸¡ì†Œ ì‹œë„ ì‹¤íŒ¨)")

        temp_df = pd.DataFrame(temp_records)

        stations = [
        ["DT_0063", "ê°€ë•ë„", 35.024, 128.81],
        ["DT_0031", "ê±°ë¬¸ë„", 34.028, 127.308],
        ["DT_0029", "ê±°ì œë„", 34.801, 128.699],
        ["DT_0026", "ê³ í¥ë°œí¬", 34.481, 127.342],
        ["DT_0018", "êµ°ì‚°", 35.975, 126.563],
        ["DT_0062", "ë§ˆì‚°", 35.197, 128.576],
        ["DT_0023", "ëª¨ìŠ¬í¬", 33.214, 126.251],
        ["DT_0007", "ëª©í¬", 34.779, 126.375],
        ["DT_0006", "ë¬µí˜¸", 37.55, 129.116],
        ["DT_0025", "ë³´ë ¹", 36.406, 126.486],
        ["DT_0005", "ë¶€ì‚°", 35.096, 129.035],
        ["DT_0061", "ì‚¼ì²œí¬", 34.924, 128.069],
        ["DT_0094", "ì„œê±°ì°¨ë„", 34.251, 125.915],
        ["DT_0010", "ì„œê·€í¬", 33.24, 126.561],
        ["DT_0022", "ì„±ì‚°í¬", 33.474, 126.927],
        ["DT_0012", "ì†ì´ˆ", 38.207, 128.594],
        ["IE_0061", "ì‹ ì•ˆê°€ê±°ì´ˆ", 33.941, 124.592],
        ["DT_0008", "ì•ˆì‚°", 37.192, 126.647],
        ["DT_0067", "ì•ˆí¥", 36.674, 126.129],
        ["DT_0037", "ì–´ì²­ë„", 36.117, 125.984],
        ["DT_0016", "ì—¬ìˆ˜", 34.747, 127.765],
        ["IE_0062", "ì˜¹ì§„ì†Œì²­ì´ˆ", 37.423, 124.738],
        ["DT_0027", "ì™„ë„", 34.315, 126.759],
        ["DT_0013", "ìš¸ë¦‰ë„", 37.491, 130.913],
        ["DT_0020", "ìš¸ì‚°", 35.501, 129.387],
        ["IE_0060", "ì´ì–´ë„", 32.122, 125.182],
        ["DT_0001", "ì¸ì²œ", 37.451, 126.592],
        ["DT_0004", "ì œì£¼", 33.527, 126.543],
        ["DT_0028", "ì§„ë„", 34.377, 126.308],
        ["DT_0021", "ì¶”ìë„", 33.961, 126.3],
        ["DT_0014", "í†µì˜", 34.827, 128.434],
        ["DT_0091", "í¬í•­", 36.051, 129.376],
        ["DT_0011", "í›„í¬", 36.677, 129.453],
        ["DT_0035", "í‘ì‚°ë„", 34.684, 125.435]
        ]
        station_df = pd.DataFrame(stations, columns=['obs_code', 'name', 'lat', 'lon'])
        sorted_stations = get_sorted_stations(station_df, lat, lon)

        url = "http://www.khoa.go.kr/api/oceangrid/tideObsSalt/search.do"
        url_with_key = f"{url}?ServiceKey={service_key}"
        sal_records = []

        for _, row in sorted_stations.iterrows():
            obs_code = row['obs_code']
            sal_records.clear()

            for date_str in sorted(set(t.strftime('%Y%m%d') for t in time_list)):
                params = {
                    "ObsCode": obs_code,
                    "Date": date_str,
                    "ResultType": "json"
                }
                r = requests.get(url_with_key, params=params)
                if not r.ok:
                    continue

                result_json = r.json()
                if result_json.get("result", {}).get("error") == "No search data":
                    continue

                for d in result_json.get("result", {}).get("data", []):
                    try:
                        sal_records.append({
                            "time": pd.to_datetime(d['record_time']),
                            "sea_water_salinity": float(d['salinity'])
                        })
                    except:
                        continue

            if sal_records:
                print(f"âœ… ì—¼ë¶„ ë°ì´í„° ì‚¬ìš© ê´€ì¸¡ì†Œ: {row['name']} ({obs_code})")
                break

        if not sal_records:
            print("âš ï¸ ì—¼ë¶„ ë°ì´í„° ì—†ìŒ (ëª¨ë“  ê´€ì¸¡ì†Œ ì‹œë„ ì‹¤íŒ¨)")

        sal_df = pd.DataFrame(sal_records)

        # 1. í•´ë¥˜ ë°ì´í„° (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        ds = uv_ds

        # 2. ê¸°ì¤€ ì‹œê°„ ë° ê³µê°„ ì •ë³´
        ds_time = pd.to_datetime(ds['time'].values).tz_localize(None).to_numpy(dtype='datetime64[ns]')
        lat_vals = ds['lat'].values
        lon_vals = ds['lon'].values

        # 3. ì¤‘ì‹¬ ì¢Œí‘œ ë° ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        center_lat = (lat_min + lat_max) / 2
        center_lon = (lon_min + lon_max) / 2
        day_time_list = pd.to_datetime(sorted(set(t.normalize() for t in time_list))).tz_localize(None)

        # 5. ì‹œê°„ ê¸°ì¤€ ìµœê·¼ì ‘ ë³´ê°„ (ê³µê°„ ë™ì¼ê°’ìœ¼ë¡œ í™•ì¥)
        def expand_to_grid(df, var_name):

            df['time'] = pd.to_datetime(df['time']).dt.tz_localize(None)
            ts = df.set_index('time')[var_name]
            if ts.index.has_duplicates:
                dup_idx = ts.index[ts.index.duplicated()].unique()
                print(f"âš ï¸ {var_name} ì¤‘ë³µ ì¸ë±ìŠ¤ ë°œê²¬! ê°œìˆ˜: {len(dup_idx)}")
                print(dup_idx)
                print(df[df['time'].isin(dup_idx)])
                ts = ts[~ts.index.duplicated(keep='first')]
            ts_interp = ts.reindex(ds_time, method='nearest', tolerance=pd.Timedelta('6H'))
            ts_interp = ts_interp.ffill().bfill()

            # (time, lat, lon) â†’ ì „ì²´ ê³µê°„ì— ë™ì¼í•œ ê°’
            var_3d = np.broadcast_to(ts_interp.values[:, np.newaxis, np.newaxis],
                                    (len(ds_time), len(lat_vals), len(lon_vals)))
            return var_3d

        temp_grid = expand_to_grid(temp_df, 'sea_water_temperature')
        sal_grid  = expand_to_grid(sal_df, 'sea_water_salinity')

        # 6. datasetì— ì‚½ì…
        ds['sea_water_temperature'] = (('time', 'lat', 'lon'), temp_grid)
        ds['sea_water_salinity'] = (('time', 'lat', 'lon'), sal_grid)

        ds['sea_water_temperature'].attrs.update(standard_name="sea_water_temperature", units="degree_Celsius")
        ds['sea_water_salinity'].attrs.update(standard_name="sea_water_salinity", units="psu")

        if not np.issubdtype(ds['time'].dtype, np.datetime64):
            ds = ds.assign_coords(time=pd.to_datetime(ds['time'].values).to_numpy(dtype='datetime64[ns]'))
        

        nc_folder = r"C:\Users\HUFS\Desktop\opendrift_middle\KHOA_nc_data"
        os.makedirs(nc_folder, exist_ok=True)

        output_path = os.path.join(nc_folder, f"{input_basename}_uv.nc")
        ds.to_netcdf(output_path)
        print(f"âœ… NetCDF ì €ì¥ ì™„ë£Œ: {output_path}")


        # ==================================
        # ğŸŒŠ 2. HYCOM API ë°ì´í„° ì²˜ë¦¬
        # ==================================

        def get_time_steps(start_dt, end_dt, max_hours=24):
            """ìš”ì²­ ì‹œê°„ì„ ìµœëŒ€ max_hours ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ (ì‹œì‘, ë) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
            time_steps = []
            current_start = start_dt
            
            # end_dtëŠ” í•­ìƒ time_list.max()ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìœ¼ë¯€ë¡œ, ë£¨í”„ëŠ” end_dtì— ë„ë‹¬í•  ë•Œê¹Œì§€ ì§„í–‰ë©ë‹ˆë‹¤.
            while current_start < end_dt:
                # ë‹¤ìŒ ìŠ¤í…ì˜ ë ì‹œê°„ = í˜„ì¬ ì‹œì‘ ì‹œê°„ + max_hours (ë‹¨, ì „ì²´ time_endë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì •)
                current_end = min(current_start + timedelta(hours=max_hours), end_dt)
                time_steps.append((current_start, current_end))
                current_start = current_end # ë‹¤ìŒ ì‹œì‘ì€ í˜„ì¬ ë ì‹œê°„ë¶€í„°

            return time_steps

        # HYCOM ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë” ì„¤ì •
        hycom_output_dir = r"C:\Users\HUFS\Desktop\opendrift_middle\hycom_data"
        os.makedirs(hycom_output_dir, exist_ok=True)
        hycom_final_filename = f"{input_basename}_hycom.nc"
        hycom_final_filepath = os.path.join(hycom_output_dir, hycom_final_filename)

        # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ì •ì˜ (uv3zëŠ” ë‹¨ì¼, surëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬)
        uv3z_filepath = os.path.join(hycom_output_dir, "temp_uv3z.nc")
        temp_sur_files = [] # ë¶„í• ëœ sur íŒŒì¼ ê²½ë¡œë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

        # ====== íŒŒì¼ ì¡´ì¬ ì‹œ ë‹¤ìš´ë¡œë“œ ë° ì²˜ë¦¬ ìƒëµ ======
        if os.path.exists(hycom_final_filepath):
            print(f"ğŸ”„ ì´ë¯¸ HYCOM NetCDF ì¡´ì¬, ë‹¤ìš´ë¡œë“œ ìƒëµ: {hycom_final_filepath}")
        else:
            try:
                data_year = time_list[0].year
            except (IndexError, AttributeError):
                print("time_listê°€ ë¹„ì–´ ìˆê±°ë‚˜ ì˜¬ë°”ë¥¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. í˜„ì¬ ì—°ë„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                data_year = datetime.now().year
            
            supported_start_year = 2018
            supported_end_year = 2024

            if not (supported_start_year <= data_year <= supported_end_year):
                print(f"ì˜¤ë¥˜: {data_year}ë…„ ë°ì´í„°ëŠ” HYCOM ì„œë²„ì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                print(f"ì§€ì›ë˜ëŠ” ì—°ë„ ë²”ìœ„: {supported_start_year}ë…„ ~ {supported_end_year}ë…„")
            else:
                # 1. ì‹œê°„ ë²”ìœ„ ì„¤ì •
                ds_time = pd.to_datetime(time_list).to_numpy(dtype='datetime64[ns]')
                time_start_dt = pd.to_datetime(ds_time.min()).replace(minute=0, second=0, microsecond=0)
                time_end_dt = pd.to_datetime(ds_time.max()).replace(minute=0, second=0, microsecond=0)
                if time_end_dt < pd.to_datetime(ds_time.max()):
                    time_end_dt += timedelta(hours=1)
                    
                time_start_str = time_start_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                time_end_str = time_end_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                
                # 2. uv3z (3ì°¨ì›) ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì „ì²´ ê¸°ê°„ í•œ ë²ˆì— ìš”ì²­)
                try:
                    print("--- 1. uv3z (3ì°¨ì›) ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ì „ì²´ ê¸°ê°„) ---")
                    uv3z_base_url = f"https://ncss.hycom.org/thredds/ncss/grid/GLBy0.08/expt_93.0/uv3z/{data_year}"
                    uv3z_params = {
                        "var": ["water_u", "water_v"],
                        "north": round(lat_max, 2), "west": math.floor(lon_min*100) / 100, "east": round(lon_max, 2), "south": math.floor(lat_min*100) / 100, 
                        "time_start": time_start_str, "time_end": time_end_str,
                        "timeStride": 1, "vertStride": 0, "accept": "netcdf4"
                    }
                    uv3z_request_url = f"{uv3z_base_url}?{urlencode(uv3z_params, doseq=True)}"
                    
                    print(uv3z_request_url)
                    response_uv3z = requests.get(uv3z_request_url, stream=True)
                    response_uv3z.raise_for_status()
                    with open(hycom_final_filepath, "wb") as f:
                        for chunk in response_uv3z.iter_content(chunk_size=8192):
                            f.write(chunk)
                    print(f"uv3z ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {hycom_final_filepath}")


                    # # 3. sur (í‘œì¸µ) ë°ì´í„° ë‹¤ìš´ë¡œë“œ (24ì‹œê°„ ë¯¸ë§Œìœ¼ë¡œ ë¶„í•  ìš”ì²­)
                    # print("\n--- 2. sur (í‘œì¸µ) ë°ì´í„° ë¶„í•  ë‹¤ìš´ë¡œë“œ (24ì‹œê°„ ë¯¸ë§Œ) ---")
                    # time_steps = get_time_steps(time_start_dt, time_end_dt, max_hours=24) # ìµœëŒ€ 24ì‹œê°„ ë‹¨ìœ„ë¡œ ë¶„í• 

                    # for i, (start_dt, end_dt) in enumerate(time_steps):
                    #     current_start_str = start_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                    #     current_end_str = end_dt.strftime("%Y-%m-%dT%H:%M:%SZ")
                        
                    #     sur_filepath = os.path.join(hycom_output_dir, f"temp_sur_{i:03d}.nc")
                    #     temp_sur_files.append(sur_filepath) # ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

                    #     print(f"  > sur êµ¬ê°„ {i+1}/{len(time_steps)} ë‹¤ìš´ë¡œë“œ: {current_start_str} ~ {current_end_str}")
                        
                    #     sur_base_url = f"https://ncss.hycom.org/thredds/ncss/GLBy0.08/expt_93.0/sur/{data_year}"
                    #     sur_params = {
                    #         "var": ["u_barotropic_velocity", "v_barotropic_velocity"],
                    #         "north": round(lat_max, 2), "west": math.floor(lon_min*100) / 100, "east": round(lon_max, 2), "south": math.floor(lat_min*100) / 100, 
                    #         "disableProjSubset": "on", "horizStride": 1, 
                    #         "time_start": current_start_str, "time_end": current_end_str, # ë¶„í• ëœ ì‹œê°„ ì‚¬ìš©
                    #         "timeStride": 1, "accept": "netcdf4"
                    #     }
                    #     sur_request_url = f"{sur_base_url}?{urlencode(sur_params, doseq=True)}"
                        
                    #     response_sur = requests.get(sur_request_url, stream=True)
                    #     response_sur.raise_for_status()
                    #     with open(sur_filepath, "wb") as f:
                    #         for chunk in response_sur.iter_content(chunk_size=8192):
                    #             f.write(chunk)
                    
                    # print(f"sur ë¶„í•  ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. ì´ {len(temp_sur_files)}ê°œ íŒŒì¼.")

                    # # 4. ë°ì´í„°ì…‹ ì—´ê¸° ë° ê²°í•©
                    # print("\n--- 3. HYCOM ë°ì´í„°ì…‹ ê²°í•© ë° ì²˜ë¦¬ ---")
                    # # uv3zëŠ” ë‹¨ì¼ íŒŒì¼ë¡œ ì—´ê³ , surëŠ” ëª¨ë“  ë¶„í•  íŒŒì¼ì„ ì—´ì–´ ë³‘í•© (open_mfdataset)
                    
                    # # **ì£¼ì˜: ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì—†ìœ¼ë©´ open_mfdatasetì´ ì‹¤íŒ¨í•˜ë¯€ë¡œ í™•ì¸**
                    # if not os.path.exists(uv3z_filepath) or not temp_sur_files:
                    #     raise Exception("ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ë¶ˆì¶©ë¶„í•˜ì—¬ ê²°í•©ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                    # with xr.open_dataset(uv3z_filepath, decode_times=True) as ds_uv3z, \
                    #     xr.open_mfdataset(temp_sur_files, combine='by_coords', decode_times=True) as ds_sur:
                        
                    #     print("ë‘ HYCOM ë°ì´í„°ì…‹ì„ ê²°í•©í•˜ì—¬ 1ì‹œê°„ ê°„ê²© ë°ì´í„° ìƒì„± ì¤‘...")
                        
                    #     # 4-1. uv3z (3ì°¨ì›) ë°ì´í„°ì—ì„œ í‘œì¸µ(depth=0.0) ìœ ì† ì¶”ì¶œ
                    #     ds_uv3z_surface = ds_uv3z.sel(depth=0.0, method='nearest').reset_coords('depth', drop=True)
                        
                    #     # 4-2. ê²½ì•• ì„±ë¶„(Baroclinic component) ê³„ì‚° ë° ë³´ê°„
                    #     # ds_surì˜ 1ì‹œê°„ ê°„ê²© ì‹œê°„ì¶•ì„ ìµœì¢… ê²°ê³¼ì˜ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
                        
                    #     # ds_uv3z_surface (3ì‹œê°„ ê°„ê²©)ì™€ ds_sur (1ì‹œê°„ ê°„ê²©)ì˜ ê³µí†µ ì‹œê°„ëŒ€(3ì‹œê°„ ê°„ê²©)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                    #     # ds_sur.timeì—ì„œ ds_uv3z_surface.timeê³¼ ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ì„ ì„ íƒí•©ë‹ˆë‹¤.
                    #     # (ì´ë¡œì¨ ds_uv3z_surfaceì™€ ds_surëŠ” 3ì‹œê°„ ê°„ê²©ì˜ ë™ì¼í•œ ì‹œê°„ì„ ê³µìœ í•˜ê²Œ ë©ë‹ˆë‹¤.)
                    #     common_times = ds_sur.time.sel(time=ds_uv3z_surface.time.values, method='nearest')
                        
                    #     # ê²½ì•• ì„±ë¶„ = í‘œì¸µ ìœ ì† (uv3z) - ìˆœì•• ìœ ì† (sur)
                    #     diff_u = ds_uv3z_surface['water_u'].sel(time=common_times) - ds_sur['u_barotropic_velocity'].sel(time=common_times)
                    #     diff_v = ds_uv3z_surface['water_v'].sel(time=common_times) - ds_sur['v_barotropic_velocity'].sel(time=common_times)
                        
                    #     # ê²½ì•• ì„±ë¶„(3ì‹œê°„ ê°„ê²©)ì„ surì˜ 1ì‹œê°„ ê°„ê²© ì‹œê°„ì¶•ì— ì„ í˜• ë³´ê°„ (1ì‹œê°„ ê°„ê²©ì˜ ê²½ì•• ì„±ë¶„ íšë“)
                    #     diff_u_interp = diff_u.interp(time=ds_sur.time, method='linear')
                    #     diff_v_interp = diff_v.interp(time=ds_sur.time, method='linear')
                        
                    #     # 4-3. ìµœì¢… ìœ ì† ê³„ì‚° (1ì‹œê°„ ê°„ê²©)
                    #     # ìµœì¢… ìœ ì† = ìˆœì•• ìœ ì† (sur, 1ì‹œê°„ ê°„ê²©) + ë³´ê°„ëœ ê²½ì•• ì„±ë¶„ (1ì‹œê°„ ê°„ê²©)
                    #     estimated_u = ds_sur['u_barotropic_velocity'] + diff_u_interp
                    #     estimated_v = ds_sur['v_barotropic_velocity'] + diff_v_interp
                        
                    #     # 4-4. ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±
                    #     ds_final = xr.Dataset(
                    #         {
                    #             "x_sea_water_velocity": (('time', 'lat', 'lon'), estimated_u.values),
                    #             "y_sea_water_velocity": (('time', 'lat', 'lon'), estimated_v.values)
                    #         },
                    #         coords={
                    #             "time": ds_sur.time.values, # ds_surì˜ 1ì‹œê°„ ê°„ê²© ì‹œê°„ ì‚¬ìš©
                    #             "lat": ds_sur.lat.values,
                    #             "lon": ds_sur.lon.values
                    #         }
                    #     )

                    #     # 5. ìµœì¢… ë°ì´í„°ì…‹ì„ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    #     ds_final.to_netcdf(hycom_final_filepath)
                    #     print(f"âœ… ë°ì´í„°ì…‹ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {hycom_final_filepath}")

                
                except requests.exceptions.RequestException as e:
                    print(f"âŒ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                except Exception as e:
                    print(f"âŒ ë°ì´í„° ì²˜ë¦¬, ê²°í•© ë˜ëŠ” ì €ì¥ ì‹¤íŒ¨: {e}")
                finally:
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    print("ğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
                    if os.path.exists(uv3z_filepath):
                        os.remove(uv3z_filepath)
                    for fpath in temp_sur_files:
                        if os.path.exists(fpath):
                            os.remove(fpath)
                    print("ğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ.")

        #=======================================================
        # PART 3: ERA5 CDS API ë°ì´í„° ë‹¤ìš´ë¡œë“œã…¡
        #=======================================================

        # ë‚ ì§œ ë²”ìœ„ ìë™ ìƒì„± (ì˜ˆ: ['09','10','11'] ë“±)
        num_days = (end_time.date() - start_time.date()).days + 1
        days = [(start_time + timedelta(days=i)).strftime("%d") for i in range(num_days)]

        # ì‹œê°„ ë¦¬ìŠ¤íŠ¸ (00:00 ~ 23:00)
        times = [f"{h:02d}:00" for h in range(24)]


        area   = [lat_max, lon_min, lat_min, lon_max]

        era5_request = {
            "product_type": ["reanalysis"],
            "variable": [
                "10m_u_component_of_wind",
                "10m_v_component_of_wind"
            ],
            "year":  [year],
            "month": [month],
            "day":   days,
            "time":  times,
            "area":  area,
            "format": "netcdf"
        }


        # ====== ì €ì¥ ê²½ë¡œ ë° íŒŒì¼ëª… =====
        print("ERA5 ìš”ì²­:", era5_request)
        client = cdsapi.Client()
        wind_folder = r"C:\Users\HUFS\Desktop\opendrift_middle\wind_data"
        os.makedirs(wind_folder, exist_ok=True)
        wind_path = os.path.join(wind_folder, f"{input_basename}_wind.nc")

        # ====== íŒŒì¼ ì¡´ì¬ ì‹œ ë‹¤ìš´ë¡œë“œ ìƒëµ ======
        if os.path.exists(wind_path):
            print(f"ğŸ”„ ì´ë¯¸ wind íŒŒì¼ ì¡´ì¬, ë‹¤ìš´ë¡œë“œ ìƒëµ: {wind_path}")
        else:
            print("ğŸŒ¬ï¸ ERA5 wind ìš”ì²­:", era5_request)
            client = cdsapi.Client()
            client.retrieve(
                'reanalysis-era5-single-levels',
                era5_request,
                wind_path
            )
            print(f"âœ… ERA5 ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {wind_path}")


        ###############################################################################
        # ê°€ì‹œê±°ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        ###############################################################################

        # ê´€ì¸¡ì†Œ ëª©ë¡ (ìœ„ë„, ê²½ë„)
        observation_stations = {
            "SF_0001": {"name": "ë¶€ì‚°í•­", "latitude": 35.091, "longitude": 129.099},
            "SF_0002": {"name": "ë¶€ì‚°í•­(ì‹ í•­)", "latitude": 35.023, "longitude": 128.808},
            "SF_0009": {"name": "í•´ìš´ëŒ€", "latitude": 35.15909, "longitude": 129.16026},
            "SF_0010": {"name": "ìš¸ì‚°í•­", "latitude": 35.501, "longitude": 129.387},
            "SF_0008": {"name": "ì—¬ìˆ˜í•­", "latitude": 34.754, "longitude": 127.752},
        }

        # API í‚¤
        service_key = 'ANM8LV6zTsRNiGg6FCUMpw=='  # ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤

        # JSON íŒŒì¼ ë¡œë“œ
        json_file = input_file

        # fishery_behaviorê°€ 1ì¸ ë°ì´í„° ì¶”ì¶œ (ì²« ë²ˆì§¸ë§Œ)
        first_fishery_behavior = None
        for feature in data['features']:
            if feature['properties']['fishery_behavior'] == 1:
                first_fishery_behavior = feature
                break  # ì²« ë²ˆì§¸ ë°ì´í„°ë§Œ ì²˜ë¦¬

        # ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œ ì°¾ê¸°
        def find_closest_station(lat, lon):
            closest_station = None
            min_distance = float('inf')

            # ê° ê´€ì¸¡ì†Œì™€ì˜ ê±°ë¦¬ ê³„ì‚°
            for obs_code, station in observation_stations.items():
                station_location = (station["latitude"], station["longitude"])
                current_location = (lat, lon)
                distance = geodesic(station_location, current_location).kilometers
                
                if distance < min_distance:
                    min_distance = distance
                    closest_station = obs_code
            
            return closest_station
        

        # ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œì—ì„œ ê°€ì‹œê±°ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        def get_visibility_from_station(obs_code, timestamp):
            # ë‚ ì§œë§Œ ì¶”ì¶œí•´ì„œ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            date_only = timestamp.split(" ")[0].replace("-", "")  # ë‚ ì§œë§Œ ì¶”ì¶œ (YYYYMMDD)

            # API ìš”ì²­ URL ìƒì„±
            url = f"http://www.khoa.go.kr/api/oceangrid/seafogReal/search.do" \
                f"?DataType=seafogReal" \
                f"&ServiceKey={service_key}" \
                f"&ObsCode={obs_code}" \
                f"&Date={date_only}" \
                f"&ResultType=json"
            
            # API ìš”ì²­
            response = requests.get(url)

            # ì‘ë‹µ ë°ì´í„° í™•ì¸
            if response.status_code == 200:
                data = response.json()
                
                # ì‘ë‹µ ë°ì´í„° ì¶œë ¥
                if 'result' in data and 'data' in data['result']:
                    closest_time_diff = float('inf')  # ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ ì°¨ì´
                    closest_visibility = None

                    for observation in data['result']['data']:
                        obs_time = observation['obs_time']

                        # ì‹œê°„ ì°¨ì´ ê³„ì‚° (ë‘ ì‹œê°„ì˜ ì°¨ì´ë¥¼ ë¶„ ë‹¨ìœ„ë¡œ ê³„ì‚°)
                        try:
                            timestamp_dt = parser.parse(timestamp.strip())
                            obs_time_dt = parser.parse(obs_time.strip())
                        except Exception as e:
                            print(f"ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: {e}")
                            continue

                        time_diff = abs((timestamp_dt - obs_time_dt).total_seconds())  # ì‹œê°„ ì°¨ì´ (ì´ˆ ë‹¨ìœ„)

                        # ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ ì°¾ê¸°
                        if time_diff < closest_time_diff:
                            closest_time_diff = time_diff
                            if 'vis' in observation:
                                closest_visibility = observation['vis']
                    
                    if closest_visibility:
                        return closest_visibility  # ê°€ì¥ ê°€ê¹Œìš´ ê°€ì‹œê±°ë¦¬ ë°˜í™˜
            return None

        # ì´ˆê¸° ë³€ìˆ˜
        visibility = None
        latitude = None
        longitude = None
        timestamp = None
        closest_station = None

        # fishery_behavior = 1ì¸ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì •ë³´ ì €ì¥
        if first_fishery_behavior:
            timestamp = first_fishery_behavior['properties']['time_stamp']
            latitude = first_fishery_behavior['properties']['latitude']
            longitude = first_fishery_behavior['properties']['longitude']
            closest_station = find_closest_station(latitude, longitude)
        else:
            print("fishery_behaviorê°€ 1ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # CSV ê²½ë¡œ ì§€ì •
        output_csv_path = r"C:\Users\HUFS\Desktop\opendrift_middle\ê°€ì‹œê±°ë¦¬csv\visibility_log_train_other.csv"

        # 1. CSVì—ì„œ í™•ì¸
        if os.path.exists(output_csv_path):
            with open(output_csv_path, mode='r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if row["filename"] == input_filename:
                        visibility = row["visibility_m"]
                        print(f"ğŸ”„ ê¸°ì¡´ CSVì—ì„œ ê°€ì‹œê±°ë¦¬ ë¶ˆëŸ¬ì˜´: {visibility}")
                        break

        # 2. ì—†ìœ¼ë©´ API í˜¸ì¶œ
        if visibility is None  and timestamp and closest_station:
            visibility = get_visibility_from_station(closest_station, timestamp)

            if visibility:
                print(f"ì‹œê°„: {timestamp} / ìœ„ì¹˜: ({latitude}, {longitude})")
                print(f"ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œ: {observation_stations[closest_station]['name']} ({closest_station})")
                print(f"ê°€ì‹œê±°ë¦¬: {visibility} ë¯¸í„°")
            else:
                print(f"ê°€ì‹œê±°ë¦¬ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 3. CSVì— ì €ì¥
        if not os.path.exists(output_csv_path):
            with open(output_csv_path, mode='w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(["filename", "visibility_m"])

        # 4. ì¤‘ë³µ ì €ì¥ ë°©ì§€ í›„ ì¶”ê°€
        already_exists = False
        with open(output_csv_path, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row["filename"] == input_filename:
                    already_exists = True
                    break

        if not already_exists:
            with open(output_csv_path, mode='a', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow([input_filename, visibility if visibility else "N/A"])



        ###############################################################################
        # (4) ERA5 windíŒŒì¼ ê²½ë¡œ
        ###############################################################################
        nc_folder = r"C:\Users\HUFS\Desktop\opendrift_middle\KHOA_nc_data"
        merged_file = os.path.join(nc_folder, f"{input_basename}_uv.nc")

        hycom_folder = r"C:\Users\HUFS\Desktop\opendrift_middle\hycom_data"
        hycom_file = os.path.join(hycom_folder, f"{input_basename}_hycom.nc")

        wind_file = os.path.join(wind_folder, f"{input_basename}_wind.nc")


        # bottom_depth = r"C:\Users\HUFS\Desktop\opendrift_middle\bottom_depth.nc"
        # print("Bottom depth file:", bottom_depth)





        ###############################################################################
        # PART 3: í•´ì•ˆì„  ì½ê¸°
        ###############################################################################
        # í•´ì•ˆì„  ì½ê¸°
        coastline_file = r"C:\Users\HUFS\Downloads\í•´ì–‘ìˆ˜ì‚°ë¶€ êµ­ë¦½í•´ì–‘ì¡°ì‚¬ì›_í•´ì•ˆì„ _20241231\2025ë…„ ì „êµ­ í•´ì•ˆì„ .shp"
        coast = gpd.read_file(coastline_file)
        if coast.crs is None or coast.crs.to_string() != 'EPSG:4326':
            coast = coast.to_crs(epsg=4326)
        coast_proj = coast.to_crs(epsg=3857)
        coastal_zone = coast_proj.buffer(15000).union_all()
        coastal_zone_wgs84 = gpd.GeoSeries(coastal_zone, crs=3857).to_crs(epsg=4326).union_all()
        print("í•´ì•ˆì„  ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")



        ###############################################################################
        # OpenDrift ëª¨ë¸ ì„¤ì • - ConnectedNetDriftë¡œ ë³€ê²½
        ###############################################################################
        class ConnectedNetDrift(OceanDrift):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.ideal_distance_m = 270  # ìë§ ì´ìƒ ê±°ë¦¬ (m)
                self.k = 0.05  # ì¡°ì • ê°•ë„ ê³„ìˆ˜ (0~1 ì‚¬ì´, ë†’ì¼ìˆ˜ë¡ ìë§ í˜•íƒœ ê°•í•¨)
                self.step = 2  # ëª‡ ê°œ ê°„ê²©ìœ¼ë¡œ ì—°ê²°í• ì§€ (3ê°œ ê°„ê²© ì—°ê²°)
                self.adjustment_loops = 2  # update ë‚´ ë°˜ë³µ ì¡°ì • íšŸìˆ˜

            def update(self):
                super().update()
                lon = self.elements.lon.copy()
                lat = self.elements.lat.copy()
                n = len(lon)

                for _ in range(self.adjustment_loops):
                    for i in range(self.step, n):
                        prev_coord = (lat[i - self.step], lon[i - self.step])
                        curr_coord = (lat[i], lon[i])
                        dist = geodesic(prev_coord, curr_coord).meters
                        delta = dist - self.ideal_distance_m

                        if abs(delta) > 0.1:
                            dlat = lat[i] - lat[i - self.step]
                            dlon = lon[i] - lon[i - self.step]
                            scale = delta / dist * self.k

                            lat[i]              -= dlat * scale
                            lon[i]              -= dlon * scale
                            lat[i - self.step]  += dlat * scale
                            lon[i - self.step]  += dlon * scale

                self.elements.lon[:] = lon
                self.elements.lat[:] = lat


        o = ConnectedNetDrift(loglevel=20)
        reader_uv = reader_netCDF_CF_generic.Reader(hycom_file)
        reader_tidal = reader_netCDF_CF_generic.Reader(merged_file)
        reader_wet   = reader_netCDF_CF_generic.Reader(wind_file)
        # reader_bathy = reader_netCDF_CF_generic.Reader(bottom_depth)
        o.add_reader(reader_uv)
        print("í•´ë¥˜ ncíŒŒì¼ ì½ê¸° ì™„ë£Œ")
        o.add_reader(reader_tidal)
        print("ìˆ˜ì¹˜ì¡°ë¥˜ë„ ncíŒŒì¼ ì½ê¸° ì™„ë£Œ")
        o.add_reader(reader_wet)
        print("ë‚ ì”¨ ncíŒŒì¼ ì½ê¸° ì™„ë£Œ")

        o.set_config('seed:wind_drift_factor', 0.02)
        o.set_config('drift:stokes_drift', True)
        o.set_config('general:seafloor_action', 'none')
        o.set_config('drift:vertical_advection', False)
        o.set_config('drift:vertical_mixing', True)
        o.set_config('general:coastline_action', 'previous')
        print("opendrift ëª¨ë¸ ì •ì˜ ì™„ë£Œ")

        ###############################################################################
        # ì…ì ì‹œë”© (ìë§ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë”©)
        df_tumang = df[df['fishery_behavior'] == 0].copy()
        df_yangmang = df[df['fishery_behavior'] == 1].copy()
        df_tumang['time_stamp'] = pd.to_datetime(df_tumang['time_stamp'])
        df_tumang = df_tumang.sort_values('time_stamp').reset_index(drop=True)
        df_yangmang = df_yangmang.sort_values('time_stamp').reset_index(drop=True)

        # ì–‘ë§ êµ¬ê°„ ì¤‘ê°„ ìœ„ì¹˜ ì¶”ì¶œ
        num_yangmang = len(df_yangmang)
        if num_yangmang == 0:
            # ì–‘ë§ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì‹œë®¬ë ˆì´ì…˜ ëª©ì ì— ë§ì§€ ì•Šìœ¼ë¯€ë¡œ ê±´ë„ˆëœ€
            # ì´ ë¶€ë¶„ì€ ê¸°ì¡´ ì½”ë“œì— ìˆì—ˆìœ¼ë¯€ë¡œ ìœ ì§€
            continue 

        # ======================== âœ¨ ì¶”ê°€ëœ í™•ì¸ ì½”ë“œ ë¸”ë¡ ì‹œì‘ âœ¨ ========================

        # íˆ¬ë§ ë°ì´í„°(ì‹œë”©í•  ì…ì)ê°€ ìˆëŠ”ì§€ í™•ì¸
        if len(df_tumang) == 0:
            print("âš ï¸ ê²½ê³ : df_tumang DataFrameì— íˆ¬ë§(0) í–‰ë™ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì…ì ì‹œë”©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue 
            
        # ======================== âœ¨ ì¶”ê°€ëœ í™•ì¸ ì½”ë“œ ë¸”ë¡ ë âœ¨ ========================

        print("ì…ì ì‹œë”© ì‹œì‘...")
        for i, row in df_tumang.iterrows():
            o.seed_elements(
                lon=row['lon'],
                lat=row['lat'],
                time=row['time_stamp'],
                z=0.0,
                origin_marker=np.array([i], dtype=np.int32)
            )
            print(f"[SEED] time={row['time_stamp']}, ìœ„ì¹˜=({row['lon']}, {row['lat']})")
        ###############################################################################
        # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        start_time_sim = df_tumang['time_stamp'].min()
        end_time_sim   = df[df['fishery_behavior'] == 1]['time_stamp'].max()
        # end_time_sim   = df_yangmang['time_stamp'].min()
        simulation_duration = end_time_sim - start_time_sim

        o.run(
            time_step=600,
            time_step_output=1800,
            duration=simulation_duration
        )


        # ###############################################################################
        # ì´ˆê¸° ë° ìµœì¢… ì…ì ìµœì¢… ìœ„ì¹˜ ì¶”ì¶œ
        num_particles = len(df_tumang)
        if num_particles == 0:
            print("âš ï¸ ê²½ê³ : íˆ¬ë§(df_tumang) ë°ì´í„°ê°€ ì—†ì–´ ì…ì ìµœì¢… ìœ„ì¹˜ ì¶”ì¶œì„ ê±´ë„ˆë›°ê³  Noneìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            start_lon_pred = start_lat_pred = end_lon_pred = end_lat_pred = None
        else:
            start_index = 0
            end_index = num_particles - 1
            
            # OpenDrift ê²°ê³¼ì—ì„œ ìœ„ì¹˜ ë°°ì—´ ì¶”ì¶œ (ì‹œë®¬ë ˆì´ì…˜ í›„ o.get_property ì‚¬ìš©)
            lon_traj, _ = o.get_property('lon')
            lat_traj, _ = o.get_property('lat')

            # 1. ì‹œì‘ ì…ì ìµœì¢… ì˜ˆì¸¡ ìœ„ì¹˜
            start_lon_pred = lon_traj[-1, start_index].item()
            start_lat_pred = lat_traj[-1, start_index].item()

            # 2. ë ì…ì ìµœì¢… ì˜ˆì¸¡ ìœ„ì¹˜
            end_lon_pred = lon_traj[-1, end_index].item()
            end_lat_pred = lat_traj[-1, end_index].item()

            print(f"[ì‹œì‘ ì…ì ìµœì¢… ì˜ˆì¸¡ ìœ„ì¹˜] lon: {start_lon_pred:.5f}, lat: {start_lat_pred:.5f}")
            print(f"[ë ì…ì ìµœì¢… ì˜ˆì¸¡ ìœ„ì¹˜] lon: {end_lon_pred:.5f}, lat: {end_lat_pred:.5f}")

        # ###############################################################################
        # ì–‘ë§ ì‹œì‘/ë ìœ„ì¹˜ ì¶”ì¶œ
        num_yangmang = len(df_yangmang)
        if num_yangmang < 2:
            print("âš ï¸ ê²½ê³ : ì–‘ë§(df_yangmang) ë°ì´í„°ê°€ 2ê°œ ë¯¸ë§Œì´ë¯€ë¡œ ì‹œì‘/ë ìœ„ì¹˜ ì¶”ì¶œì„ ê±´ë„ˆë›°ê³  Noneìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            yang_start_lon = yang_start_lat = yang_end_lon = yang_end_lat = None
        else:
            # ì–‘ë§ ì‹œì‘ ìœ„ì¹˜ (ì²« ë²ˆì§¸ í–‰)
            start_row_yangmang = df_yangmang.iloc[0]
            yang_start_lon = start_row_yangmang['lon']
            yang_start_lat = start_row_yangmang['lat']
            yang_start_time = start_row_yangmang['time_stamp']

            # ì–‘ë§ ë ìœ„ì¹˜ (ë§ˆì§€ë§‰ í–‰)
            end_row_yangmang = df_yangmang.iloc[-1]
            yang_end_lon = end_row_yangmang['lon']
            yang_end_lat = end_row_yangmang['lat']
            yang_end_time = end_row_yangmang['time_stamp']
            
            print(f"[ì–‘ë§ ì‹œì‘ ìœ„ì¹˜] time={yang_start_time}, lon={yang_start_lon:.5f}, lat={yang_start_lat:.5f}")
            print(f"[ì–‘ë§ ë ìœ„ì¹˜] time={yang_end_time}, lon={yang_end_lon:.5f}, lat={yang_end_lat:.5f}")

        # ###############################################################################
        # íˆ¬ë§ ì‹œì‘/ë ì‹œë”© ìœ„ì¹˜ ì¶”ì¶œ
        if num_particles > 0:
            start_row_tumang = df_tumang.iloc[start_index]
            end_row_tumang = df_tumang.iloc[end_index]
            tumang_start_lon = start_row_tumang['lon']
            tumang_start_lat = start_row_tumang['lat']
            tumang_end_lon = end_row_tumang['lon']
            tumang_end_lat = end_row_tumang['lat']
            
            print(f"[íˆ¬ë§ ì‹œì‘ ì‹œë”© ìœ„ì¹˜] lon={tumang_start_lon:.5f}, lat: {tumang_start_lat:.5f}")
            print(f"[íˆ¬ë§ ë ì‹œë”© ìœ„ì¹˜] lon={tumang_end_lon:.5f}, lat: {tumang_end_lat:.5f}")
        else:
            tumang_start_lon = tumang_start_lat = tumang_end_lon = tumang_end_lat = None

        # #####################################################################
        # 4ê°€ì§€ ê±°ë¦¬ ê³„ì‚°
        distance_start_pred_km = None
        distance_end_pred_km = None
        distance_tumang_start_yang_start_km = None
        distance_tumang_end_yang_end_km = None

        # ì˜ˆì¸¡ ìœ„ì¹˜ ë° ì–‘ë§ ìœ„ì¹˜ê°€ ëª¨ë‘ ìœ íš¨í•  ë•Œë§Œ ê±°ë¦¬ ê³„ì‚°
        if yang_start_lon is not None and start_lon_pred is not None:
            # ì–‘ë§ ì‹œì‘ì ê³¼ ëì 
            point_yang_start = (yang_start_lat, yang_start_lon)
            point_yang_end = (yang_end_lat, yang_end_lon)

            # 1. ì‹œì‘ ì˜ˆì¸¡ ìœ„ì¹˜ â†” ì–‘ë§ ì‹œì‘ ìœ„ì¹˜
            point_start_pred = (start_lat_pred, start_lon_pred)
            distance_start_pred_km = geodesic(point_start_pred, point_yang_start).kilometers
            print(f"[1. ì‹œì‘ ì˜ˆì¸¡ â†” ì–‘ë§ ì‹œì‘ ê±°ë¦¬] ì•½ {distance_start_pred_km:.3f} km")

            # 2. ë ì˜ˆì¸¡ ìœ„ì¹˜ â†” ì–‘ë§ ë ìœ„ì¹˜
            point_end_pred = (end_lat_pred, end_lon_pred)
            distance_end_pred_km = geodesic(point_end_pred, point_yang_end).kilometers
            print(f"[2. ë ì˜ˆì¸¡ â†” ì–‘ë§ ë ê±°ë¦¬] ì•½ {distance_end_pred_km:.3f} km")
            
            # 3. íˆ¬ë§ ì‹œì‘ ìœ„ì¹˜ â†” ì–‘ë§ ì‹œì‘ ìœ„ì¹˜
            point_tumang_start = (tumang_start_lat, tumang_start_lon)
            distance_tumang_start_yang_start_km = geodesic(point_tumang_start, point_yang_start).kilometers
            print(f"[3. íˆ¬ë§ ì‹œì‘ â†” ì–‘ë§ ì‹œì‘ ê±°ë¦¬ (ê¸°ì¤€)] ì•½ {distance_tumang_start_yang_start_km:.3f} km")

            # 4. íˆ¬ë§ ë ìœ„ì¹˜ â†” ì–‘ë§ ë ìœ„ì¹˜
            point_tumang_end = (tumang_end_lat, tumang_end_lon)
            distance_tumang_end_yang_end_km = geodesic(point_tumang_end, point_yang_end).kilometers
            print(f"[4. íˆ¬ë§ ë â†” ì–‘ë§ ë ê±°ë¦¬ (ê¸°ì¤€)] ì•½ {distance_tumang_end_yang_end_km:.3f} km")

        # ###############################################################################
        # ê°€ì‹œê±°ë¦¬ ë‹¨ìœ„ ë³€í™˜ ë° ê²°ê³¼ íŒë‹¨
        try:
            visibility_km = float(visibility) / 1000 if visibility not in (None, "N/A", "ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨") else None
        except (ValueError, TypeError):
            visibility_km = None

        # ê²°ê³¼ íŒë‹¨ (ë‘ ì˜ˆì¸¡ ê±°ë¦¬ ì¤‘ í•˜ë‚˜ë¼ë„ ê°€ì‹œê±°ë¦¬ ë‚´ì— ë“¤ì–´ì˜¤ë©´ ì„±ê³µìœ¼ë¡œ íŒë‹¨)
        prediction_result = "íŒë‹¨ ë¶ˆê°€"
        if visibility_km is not None and distance_start_pred_km is not None and distance_end_pred_km is not None:
            # ë‘ ì˜ˆì¸¡ ê±°ë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ Noneì´ ì•„ë‹Œ ìœ íš¨í•œ ê°’ë§Œ í•„í„°ë§
            valid_distances = [d for d in [distance_start_pred_km, distance_end_pred_km] if d is not None]
            
            if valid_distances:
                min_distance_km = min(valid_distances)
                prediction_result = "ì„±ê³µ" if min_distance_km < visibility_km else "ì‹¤íŒ¨"

        # ###############################################################################
        # CSV ì €ì¥

        result_csv_path = r"C:\Users\HUFS\Desktop\opendrift_middle\ì˜ˆì¸¡csv\hycom_prediction_result_train_other.csv"

        # í—¤ë” ì‘ì„±
        if not os.path.exists(result_csv_path):
            with open(result_csv_path, mode='w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow([
                    "filename", 
                    "ì‹œì‘ì˜ˆì¸¡â†”ì–‘ë§ì‹œì‘_km", 
                    "ëì˜ˆì¸¡â†”ì–‘ë§ë_km",   
                    "ê°€ì‹œê±°ë¦¬_km", 
                    "ì˜ˆì¸¡ê²°ê³¼", 
                    "íˆ¬ë§ì‹œì‘â†”ì–‘ë§ì‹œì‘_km", 
                    "íˆ¬ë§ëâ†”ì–‘ë§ë_km"   
                ])

        # íŒŒì¼ëª… ì¶”ì¶œ
        input_filename = os.path.basename(input_file)

        # ë¬¸ìì—´ ë³€í™˜ (None ëŒ€ì‘)
        dist_start_pred_str = f"{distance_start_pred_km:.3f}" if distance_start_pred_km is not None else "N/A"
        dist_end_pred_str = f"{distance_end_pred_km:.3f}" if distance_end_pred_km is not None else "N/A"
        visibility_str = f"{visibility_km:.3f}" if visibility_km is not None else "N/A"
        dist_tumang_start_yang_start_str = f"{distance_tumang_start_yang_start_km:.3f}" if distance_tumang_start_yang_start_km is not None else "N/A"
        dist_tumang_end_yang_end_str = f"{distance_tumang_end_yang_end_km:.3f}" if distance_tumang_end_yang_end_km is not None else "N/A"
        result_str = prediction_result if prediction_result is not None else "íŒë‹¨ ë¶ˆê°€"


        # ê²°ê³¼ ì“°ê¸°
        with open(result_csv_path, mode='a', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow([
                input_filename, 
                dist_start_pred_str, 
                dist_end_pred_str, 
                visibility_str, 
                result_str, 
                dist_tumang_start_yang_start_str,
                dist_tumang_end_yang_end_str
            ])

        print("âœ… CSV ì €ì¥ ì™„ë£Œ: hycom_prediction_result.csv")


        # ###############################################################################
        # ê²°ê³¼ ì‹œê°í™”
        # 1. DataFrameìœ¼ë¡œ ë³€í™˜
        df_sim = o.result[['lat', 'lon', 'origin_marker']].to_dataframe().reset_index()
        df_sim = df_sim.rename(columns={'trajectory': 'seed_id', 'time': 'timestamp'})

        # 2. ê° origin_markerì˜ ë§ˆì§€ë§‰ rowë§Œ ì„ íƒ (ë¹„í™œì„±í™” ì§ì „ ìœ„ì¹˜)
        last_df = df_sim.sort_values(['origin_marker', 'timestamp']).groupby('origin_marker').tail(1).reset_index(drop=True)

        # 3. ì¤‘ì‹¬ ì…ì ìœ„ì¹˜ (ì°¸ê³ ìš©)
        center_idx = len(last_df) // 2
        center_row = last_df.iloc[center_idx]
        print(f"ğŸ§­ ì¤‘ì‹¬ origin_marker {center_row['origin_marker']} â†’ lat: {center_row['lat']:.5f}, lon: {center_row['lon']:.5f}")

        # 4. ì‹œê°í™”
        plt.figure(figsize=(10, 7))

        # ëª¨ë“  ì…ìì˜ ê¶¤ì 
        for seed_id, group in df_sim.groupby('seed_id'):
            plt.plot(group['lon'], group['lat'], color='gray', alpha=0.4)

        # ë¹„í™œì„±í™” ì§ì „ ìœ„ì¹˜ ì—°ê²° ì„  ë° ì 
        plt.scatter(last_df['lon'], last_df['lat'], c='orange', s=10, label='ë¹„í™œì„±í™” ì§ì „ ìœ„ì¹˜')
        plt.plot(last_df['lon'], last_df['lat'], color='orange', linewidth=10, alpha=0.6, label='ë¹„í™œì„±í™” ìœ„ì¹˜ ê²½ë¡œ')

        # íˆ¬ë§(ì‹œë”©) ìœ„ì¹˜
        plt.scatter(df_tumang['lon'], df_tumang['lat'], s=30, color='blue', marker='^', label='íˆ¬ë§(0)')
        plt.scatter(df_yangmang['lon'], df_yangmang['lat'], s=30, color='red', marker='^', label='ì–‘ë§(1)')

        # ======================== âœ¨ í•µì‹¬ ì‹œê°í™” ìš”ì†Œ âœ¨ ========================

        # # ì‹¤ì œ ì–‘ë§ ì‹œì‘/ë ìœ„ì¹˜
        # if yang_start_lon is not None:
        #     # ì–‘ë§ ì‹œì‘ ìœ„ì¹˜ (X)
        #     plt.scatter(yang_start_lon, yang_start_lat, s=150, color='green', marker='X', linewidth=1, label='ì–‘ë§ ì‹œì‘ ìœ„ì¹˜')
            
        #     # ì–‘ë§ ë ìœ„ì¹˜ (P)
        #     plt.scatter(yang_end_lon, yang_end_lat, s=150, color='darkgreen', marker='P', linewidth=1, label='ì–‘ë§ ë ìœ„ì¹˜')

        #     # ì˜ˆì¸¡ ìœ„ì¹˜ ì‹œì‘/ë
        #     # ì‹œì‘ ì…ì ìµœì¢… ì˜ˆì¸¡ ìœ„ì¹˜ (*)
        #     plt.scatter(start_lon_pred, start_lat_pred, s=150, color='red', marker='*', edgecolor='black', linewidth=1, label='ì‹œì‘ ì˜ˆì¸¡ ìµœì¢… ìœ„ì¹˜')

        #     # ë ì…ì ìµœì¢… ì˜ˆì¸¡ ìœ„ì¹˜ (s)
        #     plt.scatter(end_lon_pred, end_lat_pred, s=150, color='red', marker='s', edgecolor='black', linewidth=1, label='ë ì˜ˆì¸¡ ìµœì¢… ìœ„ì¹˜')

        # ====================================================================

        # plt.title("ì‹œë®¬ë ˆì´ì…˜ ê¶¤ì  ë° ì˜ˆì¸¡ ìµœì¢… ìœ„ì¹˜ vs ì‹¤ì œ í–‰ë™ ìœ„ì¹˜")
        # plt.xlabel("Longitude")
        # plt.ylabel("Latitude")
        # plt.grid(True)
        plt.legend(loc='upper right')
        plt.tight_layout()

        # === ì‹œê°í™” ê²°ê³¼ ìë™ ì €ì¥ ===
        plot_output_dir = r"C:\Users\HUFS\Desktop\opendrift_middle\hycom_ì‹œê°í™”ê²°ê³¼_other"
        os.makedirs(plot_output_dir, exist_ok=True)
        plot_filename = f"{input_basename}.png"
        plot_path = os.path.join(plot_output_dir, plot_filename)
        plt.savefig(plot_path, dpi=300)
        plt.close()
        print(f"âœ… ì‹œê°í™” ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {plot_path}")
    
    # except Exception as e:
    #     print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {input_file} - {e}")
    #     with open(error_log_path, "a", newline="", encoding="utf-8-sig") as f:
    #         writer = csv.writer(f)
    #         writer.writerow([os.path.basename(input_file), type(e).__name__, str(e)])
    #     continue