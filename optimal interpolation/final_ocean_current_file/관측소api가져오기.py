# í•´ì•¼í•˜ëŠ” ì‘ì—…
# ê° ë‚ ì§œì— ë§ê²Œ ë²¡í„° í•©ì„±(khoa+hycom)íŒŒì¼ ì´ë¦„ ì„¤ì •í•˜ê²Œ ìë™í™”
# OI ì ìš© ì™„ë£Œ í•œ íŒŒì¼ë˜í•œ ì´ë¦„ ìë™ ì €ì¥í•˜ê²Œ ìë™í™”


import csv
import glob
import os
import requests
import xarray as xr
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, PillowWriter, FFMpegWriter
from matplotlib.widgets import Slider
from matplotlib.colors import Normalize
import pandas as pd
import json
from datetime import datetime

# ================================================================================================================================
# ================================================================================================================================
# ================================================================================================================================

# target_seqëŠ” TARGET_SEQì™€ ë™ì¼í•˜ê²Œ ì •ì˜
TARGET_SEQ = [3, 0, 3, 1, 3] 

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ì‹œí€€ìŠ¤ íƒìƒ‰ìš© í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_sequence_groups(behaviors, target=TARGET_SEQ):
    # ì—°ì† ì¤‘ë³µ ì œê±° í›„ ê·¸ë£¹ ì‹œí€€ìŠ¤ì—ì„œ ë¶€ë¶„ì—´ ìœ„ì¹˜ íƒìƒ‰
    grp = [behaviors[0]]
    for b in behaviors[1:]:
        if b != grp[-1]:
            grp.append(b)
    n,m = len(grp), len(target)
    for i in range(n-m+1):
        if grp[i:i+m] == target:
            return i, i+m
    return None

def load_df(path):
    geo = json.load(open(path, 'r', encoding='utf-8'))
    rows = [{
        'time_stamp': feat['properties']['time_stamp'],
        'fishery_behavior':   feat['properties']['fishery_behavior'],
        'lon':        feat['properties']['longitude'],
        'lat':        feat['properties']['latitude']
    } for feat in geo.get('features', [])]
    df = pd.DataFrame(rows)
    df['time_stamp'] = pd.to_datetime(df['time_stamp'], format='%Y-%m-%d %H:%M:%S') 
    return df.sort_values('time_stamp', ignore_index=True)

def locate_sequence(df):
    raw = df['fishery_behavior'].tolist()
    if len(raw) < len(TARGET_SEQ): 
        return None
    # ê·¸ë£¹ë³„ ì¸ë±ìŠ¤ ë§¤í•‘
    grp = [raw[0]]; starts=[0]; prev=raw[0]
    for i,b in enumerate(raw[1:], start=1):
        if b != prev:
            grp.append(b)
            starts.append(i)
            prev = b
    loc = find_sequence_groups(grp)
    if not loc:
        return None
    i0,i1 = loc
    start_idx = starts[i0]
    end_idx   = (starts[i1]-1) if i1<len(starts) else len(raw)-1
    return start_idx, end_idx

def seq_times(df, loc):
    s,e = loc
    return df.loc[s,'time_stamp'], df.loc[e,'time_stamp']

# â”€â”€â”€ scan_clusters ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scan_clusters():
    intervals = []
    # ëª¨ë“  íŒŒì¼ì˜ (t0,t1,fn) ìˆ˜ì§‘
    for path in glob.glob(os.path.join(geojson_dir, "*.geojson")):
        try:
            df  = load_df(path)
            loc = locate_sequence(df)
        except Exception as e:
            # JSONDecodeError (ë˜ëŠ” ê¸°íƒ€ ë¡œë“œ/íŒŒì‹± ì˜¤ë¥˜) ë°œìƒ ì‹œ ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ê³  ê±´ë„ˆëœë‹ˆë‹¤.
            if isinstance(e, json.decoder.JSONDecodeError):
                    print(f"âš ï¸ JSON í˜•ì‹ ì˜¤ë¥˜ ë°œìƒ, íŒŒì¼ ê±´ë„ˆëœ€: {os.path.basename(path)} - {e}")
            else:
                    print(f"âš ï¸ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ, íŒŒì¼ ê±´ë„ˆëœ€: {os.path.basename(path)} - {type(e).__name__}")
            continue
        if not loc: 
            continue
        t0,t1 = seq_times(df,loc)
        intervals.append((t0,t1,os.path.basename(path)))
    # ì‹œì‘ ì‹œê°„ ìˆœ ì •ë ¬
    intervals.sort(key=lambda x: x[0])
    # ê²¹ì¹˜ëŠ” êµ¬ê°„ë¼ë¦¬ ë¬¶ê¸°
    clusters = []
    cur, cur_end = [], None
    for iv in intervals:
        s,e,fn = iv
        if not cur:
            cur = [iv]; cur_end = e
        elif s <= cur_end:
            cur.append(iv)
            cur_end = max(cur_end, e)
        else:
            clusters.append(cur)
            cur = [iv]; cur_end = e
    if cur:
        clusters.append(cur)
    # í´ëŸ¬ìŠ¤í„°ë³„ ì²«/ë§ˆì§€ë§‰ íŒŒì¼
    first_list = [cluster[0][2] for cluster in clusters]
    last_list  = [cluster[-1][2] for cluster in clusters]
    return first_list, last_list

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) GeoJSON â†’ DataFrame (ì‹œë®¬ë ˆì´ì…˜ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_geojson_to_dataframe(path):
    with open(path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    records = []
    for feat in data.get('features', []):
        p = feat['properties']
        records.append({
            'time_stamp': p['time_stamp'],
            'lon':        p['longitude'],
            'lat':        p['latitude'],
            'fishery_behavior': p['fishery_behavior']
        })
    df = pd.DataFrame(records)
    df['time_stamp'] = pd.to_datetime(df['time_stamp'])
    return df.sort_values('time_stamp').reset_index(drop=True)

SERVICE_KEY = "ANM8LV6zTsRNiGg6FCUMpw=="



# 2ï¸âƒ£ ì°¾ê³  ì‹¶ì€ ìœ„ê²½ë„ ë²”ìœ„ ì„¤ì • (ì—¬ê¸°ë§Œ ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ìˆ˜ì •!)
MIN_X = 123.9609466
MAX_X = 129.22982499999998
MIN_Y = 31.82632165
MAX_Y = 35.986358300000006



data1 = [
    ["TW_0088","ê°ì²œí•­",35.052,129.003],
    ["TW_0077","ê²½ì¸í•­",37.523,126.592],
    ["TW_0089","ê²½í¬ëŒ€í•´ìˆ˜ìš•ì¥",37.808,128.931],
    ["TW_0095","ê³ ë˜ë¶ˆí•´ìˆ˜ìš•ì¥",36.58,129.454],
    ["TW_0074","ê´‘ì–‘í•­",34.859,127.792],
    ["TW_0072","êµ°ì‚°í•­",35.984,126.508],
    ["TW_0091","ë‚™ì‚°í•´ìˆ˜ìš•ì¥",38.122,128.65],
    ["KG_0025","ë‚¨í•´ë™ë¶€",34.222,128.419],
    ["TW_0069","ëŒ€ì²œí•´ìˆ˜ìš•ì¥",36.274,126.457],
    ["KG_0024","ëŒ€í•œí•´í˜‘",34.919,129.121],
    ["TW_0085","ë§ˆì‚°í•­",35.103,128.631],
    ["TW_0094","ë§ìƒí•´ìˆ˜ìš•ì¥",37.616,129.103],
    ["TW_0087","ë¶€ì‚°í•­",35.091,129.085],
    ["TW_0086","ë¶€ì‚°í•­ì‹ í•­",35.043,128.761],
    ["TW_0079","ìƒì™•ë“±ë„",35.652,126.194],
    ["TW_0081","ìƒì¼ë„",34.258,126.96],
    ["TW_0093","ì†ì´ˆí•´ìˆ˜ìš•ì¥",38.198,128.631],
    ["TW_0090","ì†¡ì •í•´ìˆ˜ìš•ì¥",35.164,129.219],
    ["TW_0083","ì—¬ìˆ˜í•­",34.794,127.808],
    ["TW_0078","ì™„ë„í•­",34.325,126.763],
    ["TW_0080","ìš°ì´ë„",34.543,125.802],
    ["KG_0101","ìš¸ë¦‰ë„ë¶ë™",38.007,131.552],
    ["KG_0102","ìš¸ë¦‰ë„ë¶ì„œ",37.742,130.601],
    ["TW_0076","ì¸ì²œí•­",37.389,126.533],
    ["TW_0092","ì„ë‘í•´ìˆ˜ìš•ì¥",35.302,129.292],
    ["KG_0021","ì œì£¼ë‚¨ë¶€",32.09,126.965],
    ["KG_0028","ì œì£¼í•´í˜‘",33.7,126.59],
    ["TW_0075","ì¤‘ë¬¸í•´ìˆ˜ìš•ì¥",33.234,126.409],
    ["TW_0082","íƒœì•ˆí•­",37.006,126.27],
    ["TW_0084","í†µì˜í•­",34.773,128.46],
    ["TW_0062","í•´ìš´ëŒ€í•´ìˆ˜ìš•ì¥",35.148,129.17]
]
data2 = [
    ["HF_0064","ê´‘ì–‘í•­",34.887,127.797],
    ["HF_0076","êµ°ì‚°í•­",35.99,126.358],
    ["HF_0041","ëŒ€í•œí•´í˜‘",34.909,129.2],
    ["HF_0073","ë™í•´ë‚¨ë¶€",36.633,130.224],
    ["HF_0075","ëª©í¬í•­ë‚´ì¸¡",34.756,126.336],
    ["HF_0074","ëª©í¬í•­ì™¸ì¸¡",34.772,126.239],
    ["HF_0040","ë¶€ì‚°í•­ì‹ í•­",35.036,128.768],
    ["HF_0065","ì—¬ìˆ˜ê´‘ì–‘í•­",34.765,127.804],
    ["HF_0039","ì—¬ìˆ˜í•´ë§Œ",34.656,127.964],
    ["HF_0063","ìš¸ì‚°í•­",35.4,129.607],
    ["HF_0069","ì¸ì²œí•­",37.355,126.508],
    ["HF_0070","íƒœì•ˆëŒ€ì‚°",37.073,126.292],
    ["HF_0071","í¬í•­í•­",36.066,129.475]
]



# ì²˜ë¦¬í•  JSON íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ
geojson_dir = r'D:\ì–´ì„ í–‰ì ë°ì´í„°\Training\02.ë¼ë²¨ë§ë°ì´í„°\TL_01.ìë§.zip (2)'
geojson_files = glob.glob(os.path.join(geojson_dir, "*.geojson"))
error_log_path = os.path.join(geojson_dir, "error_log.csv")

# HYCOM API ê¸°ë³¸ ì„¤ì • (ë³€ë™í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„°)
HYCOM_BASE_URL = "https://ncss.hycom.org/thredds/ncss/grid/GLBy0.08/expt_93.0/uv3z"
HYCOM_VARS = ["water_u", "water_v"]
HYCOM_COMMON_PARAMS = {
    "var": HYCOM_VARS,
    # ì„¤ì •í•˜ì‹  ê³ ì • ì˜ì—­ ìœ ì§€
    "north": 35.9865, "west": 123.96, "east": 129.23, "south": 31.82,
    "timeStride": 1, 
    "vertStride": 0,
    "accept": "netcdf4"
}
MAX_HYCOM_HOURS = 8 
SUPPORTED_YEARS = range(2018, 2025) 
TARGET_SEQ = [3, 0, 3, 1, 3] # ì‹œí€€ìŠ¤ íƒìƒ‰ìš©

# ì—ëŸ¬ ë¡œê·¸ ì´ˆê¸°í™”
if not os.path.exists(error_log_path):
    with open(error_log_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["íŒŒì¼ëª…", "ì˜¤ë¥˜ì¢…ë¥˜", "ì˜¤ë¥˜ë©”ì‹œì§€"])

# HYCOM ìµœì¢… ë°ì´í„° ì €ì¥ì„ ìœ„í•œ í´ë” ì„¤ì •
hycom_output_dir = r"C:\Users\HUFS\Desktop\opendrift_middle\fix_range_hycom_data"
os.makedirs(hycom_output_dir, exist_ok=True)

# ----------------------------------------------------------------------
# ğŸ“Œ ë©”ì¸ ë£¨í”„ ì‹œì‘ ì „: í´ëŸ¬ìŠ¤í„°ë§ ìŠ¤ìº” ì‹¤í–‰ (ê°€ì¥ ì¤‘ìš”í•œ í•„í„°ë§ ë‹¨ê³„)
first_list, last_list = scan_clusters()
geojson_files = list(dict.fromkeys([os.path.join(geojson_dir, fn) for fn in first_list + last_list]))
print(geojson_files)

# ğŸ—‚ï¸ [ìˆ˜ì •ë¨] ìµœì¢… ê²°ê³¼ë¬¼ì„ ì €ì¥í•  ê¸°ë³¸ í´ë” ê²½ë¡œ
BASE_OUTPUT_FOLDER_PATH = r"D:\output_files"

# ğŸ—‚ï¸ [ì¶”ê°€ë¨] ê° API ë°ì´í„°ë¥¼ ì €ì¥í•  í•˜ìœ„ í´ë” ê²½ë¡œ ì„¤ì •
BUOY_OUTPUT_FOLDER_PATH = os.path.join(BASE_OUTPUT_FOLDER_PATH, 'buoy_data')
HFRADAR_OUTPUT_FOLDER_PATH = os.path.join(BASE_OUTPUT_FOLDER_PATH, 'hfradar_data')



# ğŸ—‚ï¸ [ì¶”ê°€ë¨] ê²°ê³¼ë¬¼ ì €ì¥ í´ë”ë“¤ì´ ì—†ìœ¼ë©´ ëª¨ë‘ ìƒì„±
os.makedirs(BUOY_OUTPUT_FOLDER_PATH, exist_ok=True)
os.makedirs(HFRADAR_OUTPUT_FOLDER_PATH, exist_ok=True)


# ==============================================================================
# ğŸ“ 2. JSON íŒŒì¼ ì‹œê°„ ì¶”ì¶œ í•¨ìˆ˜ (ğŸš¨ ì‹¤ì œ íŒŒì¼ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
# ==============================================================================
def extract_time_from_json(json_file_path):
    """
    JSON íŒŒì¼ì„ ì½ì–´ ì‹œì‘ê³¼ ë ì‹œê°„ ì •ë³´ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
    """
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            # [ìˆ˜ì •ë¨] ë³´ë‚´ì£¼ì‹  JSON êµ¬ì¡°ì— ë§ê²Œ 'crs' ê°ì²´ì—ì„œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ
            crs_data = data.get('crs', {})
            start_date_str = crs_data.get('start_time') # "2022-03-12 05:09:00"
            end_date_str = crs_data.get('end_time')     # "2022-03-13 05:08:00"
            
            if not all([start_date_str, end_date_str]):
                print(f"  - âŒ ì˜¤ë¥˜: JSON íŒŒì¼ì—ì„œ 'start_time' ë˜ëŠ” 'end_time'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, None

            # [ìˆ˜ì •ë¨] ë‚ ì§œ ë¬¸ìì—´ í˜•ì‹ì— ë§ëŠ” í¬ë§·ìœ¼ë¡œ datetime ê°ì²´ ë³€í™˜
            date_format = "%Y-%m-%d %H:%M:%S"
            start_date = datetime.strptime(start_date_str, date_format)
            end_date = datetime.strptime(end_date_str, date_format)
            
            return start_date, end_date

    except json.JSONDecodeError:
        print(f"  - âŒ ì˜¤ë¥˜: '{os.path.basename(json_file_path)}'ëŠ” ì˜¬ë°”ë¥¸ JSON íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤.")
        return None, None
    except Exception as e:
        print(f"  - âŒ '{os.path.basename(json_file_path)}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return None, None




# ==============================================================================
# ğŸ” 3. ë©”ì¸ ì²˜ë¦¬ ë¡œì§
# ==============================================================================

# ê° JSON íŒŒì¼ì— ëŒ€í•´ ì‘ì—… ë°˜ë³µ
for geojson_filename in geojson_files:
    json_file_full_path = os.path.join(geojson_dir, geojson_filename)
    print(f"\n{'='*80}\nâ–¶ï¸ ì‘ì—… ì‹œì‘: {geojson_filename}\n{'='*80}")
    # 1. JSONì—ì„œ ì‹œê°„ ì •ë³´ ì¶”ì¶œ
    START_DATE, END_DATE = extract_time_from_json(json_file_full_path)

    if START_DATE is None:
        print(f"âš ï¸ ì‹œê°„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ '{geojson_filename}' íŒŒì¼ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue # ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ì–´ê°

    print(f"  - ì¶”ì¶œëœ ê¸°ê°„: {START_DATE} ~ {END_DATE}")
    
    geojson_basename = os.path.basename(geojson_filename)
    base_json_name = os.path.splitext(geojson_basename)[0]
    khoa_buoy_OUTPUT_FILENAME = os.path.join(BUOY_OUTPUT_FOLDER_PATH, f"{base_json_name}_buoy.nc")
    khoa_hfradar_output_filename = os.path.join(HFRADAR_OUTPUT_FOLDER_PATH, f"{base_json_name}_hfradar.nc")


    ########################################################
    # í•´ìƒë¶€ì´, ê´€ì¸¡ì†Œ api ëŒê³ ì˜¤ê¸°
    ########################################################


    # ë°ì´í„° í•„í„°ë§ ë° STATIONS ë”•ì…”ë„ˆë¦¬ ìƒì„±
    STATIONS1 = {}
    for item in data1:
        # item: [ID, Name, Latitude (Y), Longitude (X)]
        station_id, name, lat, lon = item[0], item[1], item[2], item[3]

        # ê²½ë„ (lon)ì™€ ìœ„ë„ (lat)ê°€ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
        is_in_x_range = MIN_X <= lon <= MAX_X
        is_in_y_range = MIN_Y <= lat <= MAX_Y
        
        if is_in_x_range and is_in_y_range:
            STATIONS1[station_id] = {
                'name': name,
                'lat': lat,
                'lon': lon
            }


    # ë°ì´í„° í•„í„°ë§ ë° STATIONS ë”•ì…”ë„ˆë¦¬ ìƒì„±
    STATIONS2 = {}
    for item in data2:
        # item: [ID, Name, Latitude (Y), Longitude (X)]
        station_id, name, lat, lon = item[0], item[1], item[2], item[3]

        # ê²½ë„ (lon)ì™€ ìœ„ë„ (lat)ê°€ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
        is_in_x_range = MIN_X <= lon <= MAX_X
        is_in_y_range = MIN_Y <= lat <= MAX_Y
        
        if is_in_x_range and is_in_y_range:
            STATIONS2[station_id] = {
                'name': name,
                'lat': lat,
                'lon': lon
            }


    from datetime import datetime, timedelta

    # ==============================================================================
    # ì„¤ì • ë¶€ë¶„ // ìœ„ê²½ë„ì— ë§ê²Œ ë²”ìœ„ ë‚´ì— ìˆëŠ” ê´€ì¸¡ì†Œì™€ í•´ìƒë¶€ì´ api ëŒê³ ì˜¤ê¸°
    # ==============================================================================


    # API ê¸°ë³¸ URL
    BUOY_BASE_URL  = "http://www.khoa.go.kr/api/oceangrid/tidalBu/search.do"





    # ==============================================================================
    # í•´ìƒë¶€ìœ„ ë¡œì§
    # ==============================================================================

    if os.path.exists(khoa_buoy_OUTPUT_FILENAME):
        print(f"ğŸ”„ ì´ë¯¸ NetCDF íŒŒì¼ì´ ì¡´ì¬í•˜ì—¬ ë‹¤ìš´ë¡œë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤: {khoa_buoy_OUTPUT_FILENAME}")
    else:
        all_records = []
        
        # APIëŠ” í•˜ë£¨ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì œê³µí•˜ë¯€ë¡œ, ë‚ ì§œ ëª©ë¡ ìƒì„±
        date_list = pd.date_range(start=START_DATE.date(), end=END_DATE.date()).tolist()

        # ====== API í˜¸ì¶œ ë° ë°ì´í„° ìˆ˜ì§‘ ======
        for station_id, station_info in STATIONS1.items():
            for search_date in date_list:
                print(f"ğŸ“¡ {station_info['name']}({station_id}) ê´€ì¸¡ì†Œì˜ {search_date.strftime('%Y-%m-%d')} ë°ì´í„° ìš”ì²­ ì¤‘...")
                
                params = {
                    'ServiceKey': SERVICE_KEY,
                    'ObsCode': station_id,
                    'Date': search_date.strftime('%Y%m%d'),
                    'ResultType': 'json'
                }
                
                try:
                    response = requests.get(BUOY_BASE_URL , params=params, timeout=10)
                    
                    if response.status_code == 200:
                        data = response.json()
                        # ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                        if 'result' in data and 'data' in data['result']:
                            for record in data['result']['data']:
                                all_records.append({
                                    'station_id': station_id,
                                    # 'station_name': station_info['name'],
                                    'lat': station_info['lat'],
                                    'lon': station_info['lon'],
                                    'time': pd.to_datetime(record['obs_time']),
                                    'current_speed_cm_s': pd.to_numeric(record['current_speed'], errors='coerce'),
                                    'current_direction_deg': pd.to_numeric(record['current_direct'], errors='coerce')
                                })
                        else:
                            print(f"   âš ï¸ ë°ì´í„° ì—†ìŒ: {station_info['name']} ({search_date.strftime('%Y-%m-%d')})")

                    else:
                        print(f"   âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {station_info['name']}, Status Code: {response.status_code}")

                except requests.exceptions.RequestException as e:
                    print(f"   âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {station_info['name']}, {e}")
                except Exception as e:
                    print(f"   âŒ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {station_info['name']}, {e}")
        
        if not all_records:
            print("ğŸ˜­ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ NetCDF íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ====== ë°ì´í„°í”„ë ˆì„ ë³€í™˜ ë° í•„í„°ë§ ======
            df = pd.DataFrame(all_records)
            df.dropna(inplace=True) # ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°

            # ì‹œê°„ ë²”ìœ„ ë° ì •ê° ë°ì´í„° í•„í„°ë§
            df_hourly = df[
                (df['time'] >= START_DATE) & 
                (df['time'] <= END_DATE) & 
                (df['time'].dt.minute == 0)
            ].copy()

            print(f"\nğŸ“Š ì´ {len(df_hourly)}ê°œì˜ ì‹œê°„ë³„ ê´€ì¸¡ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ.")

            # ====== NetCDF ìƒì„± ======
            # xarrayê°€ ì¸ì‹í•˜ê¸° ì¢‹ê²Œ station_idë¥¼ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
            df_hourly.drop_duplicates(subset=['station_id', 'time'], inplace=True, keep='first')

            # ì´ì œ ì¤‘ë³µì´ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            df_hourly.set_index(['station_id', 'time'], inplace=True)
            ds = df_hourly.to_xarray()
            
            # ê° ë³€ìˆ˜ì— ëŒ€í•œ ì†ì„±(ì„¤ëª…, ë‹¨ìœ„) ì¶”ê°€
            # ds['station_name'].attrs = {'long_name': 'Observation station name'}
            ds['lat'].attrs = {'long_name': 'Latitude', 'units': 'degrees_north'}
            ds['lon'].attrs = {'long_name': 'Longitude', 'units': 'degrees_east'}
            ds['current_speed_cm_s'].attrs = {'long_name': 'Sea water speed', 'units': 'cm/s'}
            ds['current_direction_deg'].attrs = {'long_name': 'Sea water direction (from north)', 'units': 'degree'}

            # íŒŒì¼ ì „ì²´ì— ëŒ€í•œ ì „ì—­ ì†ì„± ì¶”ê°€
            ds.attrs = {
                'title': 'KHOA Oceanographic Buoy Data',
                'source': 'Korea Hydrographic and Oceanographic Agency (KHOA)',
                'api': 'tidalBu (í•´ì–‘ê´€ì¸¡ë¶€ì´)',
                'description': 'Hourly current speed and direction data from KHOA buoys.',
                'history': f'Created on {datetime.now().isoformat()}'
            }
            
            # NetCDF íŒŒì¼ë¡œ ì €ì¥
            print(f"\nâœ… NetCDF íŒŒì¼ ìƒì„± ì™„ë£Œ: {khoa_buoy_OUTPUT_FILENAME}")
            ds.to_netcdf(khoa_buoy_OUTPUT_FILENAME)
            
            
    # ==============================================================================
    # í•´ìƒ ê´€ì¸¡ì†Œ ë¡œì§
    # ==============================================================================        
            
    # HF-RADAR API ê¸°ë³¸ ì •ë³´
    HFRADAR_BASE_URL = "http://www.khoa.go.kr/api/oceangrid/tidalHfRadar/search.do"

    START_DATE = START_DATE.replace(minute=0, second=0, microsecond=0)
    END_DATE = END_DATE.replace(minute=0, second=0, microsecond=0)
    # ë°ì´í„° ì¡°íšŒ ê¸°ê°„ ì„¤ì • (ì´ì „ê³¼ ë™ì¼)
    time_list = pd.date_range(start=START_DATE, end=END_DATE, freq='H')


    # ==================================
    # 2. ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
    # ==================================
    if os.path.exists(khoa_hfradar_output_filename):
        print(f"ğŸ”„ ì´ë¯¸ NetCDF íŒŒì¼ì´ ì¡´ì¬í•˜ì—¬ ë‹¤ìš´ë¡œë“œë¥¼ ìƒëµí•©ë‹ˆë‹¤: {khoa_hfradar_output_filename}")
    else:
        all_records = []

        print("ğŸš€ HF-RADAR API ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        # ëª¨ë“  ê´€ì¸¡ì†Œì™€ ì‹œê°„ì— ëŒ€í•´ API í˜¸ì¶œ
        for obs_code, station_name in STATIONS2.items():
            for t in time_list:
                # Date íŒŒë¼ë¯¸í„° í˜•ì‹ì´ YYYYMMDDHH ì…ë‹ˆë‹¤.
                date_str = t.strftime("%Y%m%d%H")
                
                params = {
                    'ServiceKey': SERVICE_KEY,
                    'ObsCode': obs_code,
                    'Date': date_str,
                    'ResultType': 'json'
                }
                
                try:
                    response = requests.get(HFRADAR_BASE_URL, params=params)
                    if response.status_code == 200:
                        data = response.json()
                        # 'data' í‚¤ê°€ ìˆëŠ”ì§€, ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
                        if 'result' in data and 'data' in data['result'] and data['result']['data']:
                            # í•œ ë²ˆì˜ í˜¸ì¶œë¡œ ì—¬ëŸ¬ ìœ„ì¹˜ì˜ ë°ì´í„°ê°€ ë“¤ì–´ì˜´
                            for record in data['result']['data']:
                                all_records.append({
                                    'time': t,
                                    'station_id': obs_code,
                                    # 'station_name': station_name,
                                    'lat': float(record.get('lat', float('nan'))),
                                    'lon': float(record.get('lon', float('nan'))),
                                    'current_speed': float(record.get('current_speed', float('nan'))), # cm/s
                                    'current_direct': float(record.get('current_direct', float('nan'))) # deg
                                })
                            print(f"âœ… [{station_name}({obs_code})] {t.strftime('%Y-%m-%d %H:%M')} ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ")
                        else:
                            print(f"âš ï¸ [{station_name}({obs_code})] {t.strftime('%Y-%m-%d %H:%M')} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        print(f"âŒ [{station_name}({obs_code})] {t.strftime('%Y-%m-%d %H:%M')} API í˜¸ì¶œ ì‹¤íŒ¨ (ìƒíƒœ ì½”ë“œ: {response.status_code})")
                except requests.exceptions.RequestException as e:
                    print(f"âŒ [{station_name}({obs_code})] {t.strftime('%Y-%m-%d %H:%M')} ìš”ì²­ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                except ValueError: # JSON ë””ì½”ë”© ì˜¤ë¥˜ ì²˜ë¦¬
                    print(f"âŒ [{station_name}({obs_code})] {t.strftime('%Y-%m-%d %H:%M')} JSON íŒŒì‹± ì‹¤íŒ¨. ì‘ë‹µ ë‚´ìš©: {response.text}")


        if not all_records:
            print("ğŸ˜­ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ NetCDF íŒŒì¼ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ==================================
            # 3. NetCDF ìƒì„±
            # ==================================
            print("\nğŸ“Š NetCDF íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
            
            df = pd.DataFrame(all_records)
            # ì¤‘ë³µ ë°ì´í„° ì œê±° (ì•ˆì •ì„± í™•ë³´)
            df.drop_duplicates(subset=['time', 'lat', 'lon'], inplace=True, keep='first')
            
            # ë°ì´í„°í”„ë ˆì„ì„ xarray Datasetìœ¼ë¡œ ë³€í™˜
            # HF-Radar ë°ì´í„°ëŠ” ê° ì‹œê°„ì´ ê³µê°„ ê²©ìë¥¼ ê°€ì§€ë¯€ë¡œ, ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥
            ds = df.set_index('time').to_xarray()
            
            # ë³€ìˆ˜ ì†ì„±(metadata) ì¶”ê°€
            ds['current_speed'].attrs = {'long_name': 'Sea Water Speed', 'units': 'cm/s'}
            ds['current_direct'].attrs = {'long_name': 'Sea Water To Direction', 'units': 'degree'}
            ds['lat'].attrs = {'units': 'degrees_north'}
            ds['lon'].attrs = {'units': 'degrees_east'}
            ds['station_id'].attrs = {'long_name': 'Observation station code'}
            
            # ì „ì—­ ì†ì„±(Global attributes) ì¶”ê°€
            ds.attrs = {
                'title': 'KHOA HF-RADAR Ocean Current Data',
                'source': 'Korea Hydrographic and Oceanographic Agency (KHOA)',
                'api_url': 'http://www.khoa.go.kr/api/oceangrid/tidalHfRadar/search.do',
                'history': f'Created on {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'
            }

            # NetCDF íŒŒì¼ë¡œ ì €ì¥
            ds.to_netcdf(khoa_hfradar_output_filename)
            print(f"âœ… NetCDF ìƒì„± ì™„ë£Œ: {khoa_hfradar_output_filename}")
            
            
        print(f"\nğŸ‰ {geojson_filename}ì— ëŒ€í•œ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

print(f"\n{'='*80}\nâœ… ëª¨ë“  GeoJSON íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n{'='*80}")
        