import math
import os
from urllib.parse import urlencode
#â€“â€“ PROJ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (pyproj â‰¥3ìš© PROJ_DATA í¬í•¨)
os.environ['PROJ_LIB']  = r'C:\Users\HUFS\anaconda3\envs\opendrift_env\Library\share\proj'
os.environ['PROJ_DATA'] = r'C:\Users\HUFS\anaconda3\envs\opendrift_env\Library\share\proj'

import json
import glob
import time
from datetime import datetime, timedelta


import csv
from dateutil import parser
import numpy as np
import pandas as pd
import xarray as xr
import cdsapi
import requests
import geopandas as gpd
from geopy.distance import geodesic
from shapely.geometry import Point, LineString

import matplotlib.pyplot as plt
from scipy.interpolate import griddata

from opendrift.models.oceandrift import OceanDrift
from opendrift.readers import reader_netCDF_CF_generic
from collections import OrderedDict

import matplotlib.pyplot as plt
import matplotlib as mpl

plt.rcParams['font.family'] = 'Malgun Gothic'
mpl.rcParams['axes.unicode_minus'] = False


TARGET_SEQ = [3,0,3,1,3]   # ì‹œí€€ìŠ¤ íƒìƒ‰ìš© (ì „ì—­ ë³€ìˆ˜ë¡œ ì´ë™)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) ì‹œí€€ìŠ¤ íƒìƒ‰ìš© í—¬í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_sequence_groups(behaviors, target=TARGET_SEQ):
    """ì—°ì† ì¤‘ë³µì´ ì œê±°ëœ ë¦¬ìŠ¤íŠ¸ì—ì„œ íƒ€ê²Ÿ ì‹œí€€ìŠ¤ì˜ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    grp = [behaviors[0]]
    for b in behaviors[1:]:
        if b != grp[-1]:
            grp.append(b)
    n,m = len(grp), len(target)
    for i in range(n-m+1):
        if grp[i:i+m] == target:
            return i, i+m # ì••ì¶•ëœ ë¦¬ìŠ¤íŠ¸ì—ì„œì˜ ì‹œì‘, ë ì¸ë±ìŠ¤
    return None

def load_df(path):
    """ì‹œí€€ìŠ¤ ìŠ¤ìº”ì„ ìœ„í•œ ìµœì†Œí•œì˜ DataFrame ë¡œë” (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)"""
    geo = json.load(open(path, 'r', encoding='utf-8'))
    rows = []
    for feat in geo.get('features', []):
        p = feat['properties']
        rows.append({
            'time_stamp': p.get('time_stamp'),
            'fishery_behavior': p.get('fishery_behavior'),
        })
    df = pd.DataFrame(rows)
    # ì›ë³¸ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•˜ê²Œ errors='coerce'ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
    df['time_stamp'] = pd.to_datetime(df['time_stamp'], errors='coerce') 
    return df.sort_values('time_stamp', ignore_index=True)

def locate_sequence(df):
    """DataFrameì—ì„œ TARGET_SEQì˜ *ì›ë³¸ ì¸ë±ìŠ¤* ìœ„ì¹˜(ì‹œì‘, ë)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    raw = df['fishery_behavior'].tolist()
    if len(raw) < len(TARGET_SEQ): 
        return None
        
    # ê·¸ë£¹ë³„ ì¸ë±ìŠ¤ ë§¤í•‘ (ì••ì¶•ëœ ë¦¬ìŠ¤íŠ¸ì˜ ì¸ë±ìŠ¤ê°€ ì›ë³¸ ë¦¬ìŠ¤íŠ¸ì˜ ëª‡ ë²ˆì§¸ ì¸ë±ìŠ¤ì—ì„œ ì‹œì‘ëëŠ”ì§€ ì¶”ì )
    grp = [raw[0]]; starts=[0]; prev=raw[0]
    for i,b in enumerate(raw[1:], start=1):
        if b != prev:
            grp.append(b)
            starts.append(i) # ìƒˆë¡œìš´ ê·¸ë£¹(b)ì´ ì‹œì‘ëœ ì›ë³¸ ì¸ë±ìŠ¤(i)
            prev = b
            
    loc = find_sequence_groups(grp) # ì••ì¶• ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€ (i0, i1)
    if not loc:
        return None
        
    i0, i1 = loc
    start_idx = starts[i0] # ì‹œí€€ìŠ¤ ì‹œì‘ ê·¸ë£¹ì˜ ì›ë³¸ ì¸ë±ìŠ¤
    
    # ì‹œí€€ìŠ¤ ë ê·¸ë£¹ì˜ ì›ë³¸ *ë§ˆì§€ë§‰* ì¸ë±ìŠ¤
    # (i1ì€ ëë‚˜ëŠ” ê·¸ë£¹ì˜ ë‹¤ìŒ ì¸ë±ìŠ¤ì´ë¯€ë¡œ, starts[i1]-1ì´ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ê°€ ë¨)
    end_idx = (starts[i1]-1) if i1 < len(starts) else len(raw)-1
    
    return start_idx, end_idx

def seq_times(df, loc):
    """loc (ì‹œì‘, ë ì¸ë±ìŠ¤)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ì‹œì‘/ë íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    s,e = loc
    return df.loc[s,'time_stamp'], df.loc[e,'time_stamp']

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) í´ëŸ¬ìŠ¤í„° ìŠ¤ìºë„ˆ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scan_clusters(file_list):
    """
    ëª¨ë“  íŒŒì¼ì˜ ì‹œí€€ìŠ¤ ì‹œê°„(t0, t1)ì„ ìˆ˜ì§‘í•˜ê³ ,
    ê²¹ì¹˜ëŠ” êµ¬ê°„ì„ í´ëŸ¬ìŠ¤í„°ë¡œ ë¬¶ì–´ ì²«/ë§ˆì§€ë§‰ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    intervals = []
    print(f"ğŸ”¬ {len(file_list)}ê°œ ì „ì²´ íŒŒì¼ ìŠ¤ìº” ì‹œì‘...")
    
    # 1. ëª¨ë“  íŒŒì¼ì˜ (t0, t1, fn) ìˆ˜ì§‘
    for path in file_list:
        fn = os.path.basename(path)
        try:
            df  = load_df(path)
            loc = locate_sequence(df)
        except Exception as e:
            if isinstance(e, json.decoder.JSONDecodeError):
                print(f"  âš ï¸ JSON í˜•ì‹ ì˜¤ë¥˜, ìŠ¤ìº” ì œì™¸: {fn}")
            else:
                print(f"  âš ï¸ ìŠ¤ìº” ì¤‘ ì˜¤ë¥˜, ì œì™¸: {fn} - {type(e).__name__}")
            continue
            
        if not loc: 
            # print(f"  - ì‹œí€€ìŠ¤ ì—†ìŒ, ìŠ¤ìº” ì œì™¸: {fn}") # (ë¡œê·¸ê°€ ë„ˆë¬´ ë§ì•„ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬)
            continue
            
        t0,t1 = seq_times(df,loc)
        intervals.append((t0,t1,fn))
    
    print(f"  âœ… ì‹œí€€ìŠ¤ í¬í•¨ íŒŒì¼ {len(intervals)}ê°œ ë°œê²¬.")
    
    # 2. ì‹œì‘ ì‹œê°„ ìˆœ ì •ë ¬
    intervals.sort(key=lambda x: x[0])
    
    # 3. ê²¹ì¹˜ëŠ” êµ¬ê°„ë¼ë¦¬ ë¬¶ê¸° (í´ëŸ¬ìŠ¤í„°ë§)
    clusters = []
    cur, cur_end = [], None
    for iv in intervals:
        s,e,fn = iv
        if not cur: # ì²« ë²ˆì§¸ í´ëŸ¬ìŠ¤í„° ì‹œì‘
            cur = [iv]; cur_end = e
        elif s <= cur_end: # í˜„ì¬ í´ëŸ¬ìŠ¤í„°ì™€ ì‹œê°„ì´ ê²¹ì¹¨
            cur.append(iv)
            cur_end = max(cur_end, e) # í´ëŸ¬ìŠ¤í„°ì˜ ë ì‹œê°„ ê°±ì‹ 
        else: # ìƒˆë¡œìš´ í´ëŸ¬ìŠ¤í„° ì‹œì‘
            clusters.append(cur)
            cur = [iv]; cur_end = e
    if cur:
        clusters.append(cur)
        
    print(f"  ğŸ“Š ì´ {len(clusters)}ê°œì˜ ì¡°ì—… í´ëŸ¬ìŠ¤í„°ë¡œ ê·¸ë£¹í™” ì™„ë£Œ.")

    # 4. í´ëŸ¬ìŠ¤í„°ë³„ ì²«/ë§ˆì§€ë§‰ íŒŒì¼ëª… ì¶”ì¶œ
    first_list = [cluster[0][2] for cluster in clusters]
    last_list  = [cluster[-1][2] for cluster in clusters]
    
    return first_list, last_list

# ==============================================================================
# ğŸ—‚ï¸ 2. í™˜ê²½ ì„¤ì • ë° ë©”ì¸ ë£¨í”„
# ==============================================================================

geojson_dir = r"D:\ì–´ì„ í–‰ì ë°ì´í„°\Training\02.ë¼ë²¨ë§ë°ì´í„°\TL_01.ìë§.zip"
# geojson_dir = r'D:\ì–´ì„ í–‰ì ë°ì´í„°\Validation\02.ë¼ë²¨ë§ë°ì´í„°\VL_01.ìë§.zip'
geojson_files = glob.glob(os.path.join(geojson_dir, "*.geojson"))


# âœ¨ (ìˆ˜ì •) 1. ë¨¼ì € ëª¨ë“  íŒŒì¼ì„ globìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤.
all_files = glob.glob(os.path.join(geojson_dir, "*.geojson"))

# âœ¨ (ìˆ˜ì •) 2. ìŠ¤ìº” í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ í•„í„°ë§í•©ë‹ˆë‹¤.
first_list, last_list = scan_clusters(all_files)

# âœ¨ (ìˆ˜ì •) 3. í•„í„°ë§ëœ íŒŒì¼ ëª©ë¡(first + last)ìœ¼ë¡œ geojson_files ë³€ìˆ˜ë¥¼ ìƒˆë¡œ ì •ì˜í•©ë‹ˆë‹¤.
# dict.fromkeysë¥¼ ì‚¬ìš©í•´ ì¤‘ë³µ(í´ëŸ¬ìŠ¤í„°ê°€ íŒŒì¼ 1ê°œì¸ ê²½ìš°)ì„ ì œê±°í•©ë‹ˆë‹¤.
filtered_files_names = list(dict.fromkeys(first_list + last_list))
geojson_files = [os.path.join(geojson_dir, fn) for fn in filtered_files_names]

print(f"ğŸ‰ í•„í„°ë§ ì™„ë£Œ! ì „ì²´ {len(all_files)}ê°œ íŒŒì¼ ì¤‘ {len(geojson_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘.")
print("-" * 60)


error_log_path = os.path.join(geojson_dir, "error_log.csv")

# ì—ëŸ¬ ë¡œê·¸ ì´ˆê¸°í™”
if not os.path.exists(error_log_path):
    with open(error_log_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["íŒŒì¼ëª…", "ì˜¤ë¥˜ì¢…ë¥˜", "ì˜¤ë¥˜ë©”ì‹œì§€"])

# ì „ì²´ ìë™ ì²˜ë¦¬ ë£¨í”„
for input_file in geojson_files:
    try:
        input_filename = os.path.basename(input_file)
        visibility = None            # ê°€ì‹œê±°ë¦¬(m)
        distance_km = None           # ì¤‘ê°„ íˆ¬ë§ â†” ì–‘ë§ ê±°ë¦¬(km)
        prediction_result = "íŒë‹¨ ë¶ˆê°€"  # ì˜ˆì¸¡ ì„±ê³µ ì—¬ë¶€

        print(f"\n===== ì²˜ë¦¬ ì‹œì‘: {input_filename} =====")

        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)


        ###############################################################################
        # PART 1: GeoJSON â†’ DataFrame (íˆ¬ë§ ê¶¤ì  ì¶”ì¶œ)
        ###############################################################################
        rows = []
        for feat in data.get("features", []):
            p = feat["properties"]
            beh = p.get("fishery_behavior")
            rows.append({
                "time_stamp": p["time_stamp"],
                "lon":         p["longitude"],
                "lat":         p["latitude"],
                "fishery_behavior": beh
            })

        df = pd.DataFrame(rows)
        if df.empty:
            print(f"âš ï¸ {input_filename}: DataFrame ë¹„ì–´ìˆìŒ â†’ ê±´ë„ˆëœ€")
            continue

        print(f"ğŸ“Š {input_filename}: DataFrame í¬ê¸° = {df.shape}")

        df['time_stamp'] = pd.to_datetime(df['time_stamp'], errors='coerce')
        df = df.sort_values('time_stamp').reset_index(drop=True)

        # íˆ¬ë§ ì‹œì‘ ì§€ì ë§Œ í•„í„°ë§ (1->3 ë˜ëŠ” 0->3 ë³€í™” ì‹œì )
        df['prev_behavior'] = df['fishery_behavior'].shift(1)
        drop_points = df[
            (df['fishery_behavior'] == 3) &
            (df['prev_behavior'] != 3)
        ].copy()

        print(f"ğŸ¯ drop_points ê°œìˆ˜: {len(drop_points)}")

        # ğŸ“Œ 2. ì‹œê°„ ë¦¬ìŠ¤íŠ¸ (1ì‹œê°„ ê°„ê²©)
        start_time = df['time_stamp'].min().replace(minute=0, second=0)
        end_time = df['time_stamp'].max()
        print(f"ğŸ•’ start_time={start_time}, end_time={end_time}")

        time_list = []
        current_time = start_time
        while current_time <= end_time:
            time_list.append(current_time)
            current_time += timedelta(hours=1)

        print(f"ğŸ•’ time_list ê¸¸ì´: {len(time_list)} â†’ {time_list[:5]}...")

        simulation_duration = end_time - start_time
        print(f"â³ simulation_duration: {simulation_duration}")

        # ì—°, ì›”, ì¼ ë¬¸ìì—´ë¡œ ì¶”ì¶œ
        year  = f"{start_time.strftime('%Y')}"
        month = f"{start_time.strftime('%m')}"
        day   = f"{start_time.strftime('%d')}"
        print(f"ğŸ“… ë‚ ì§œ: {year}-{month}-{day}")

        lat_min = 31.82632165
        lat_max = 35.9863583
        lon_min = 123.9609466
        lon_max = 129.2298249

        print(f"ğŸŒ lat ë²”ìœ„=({lat_min}, {lat_max}), lon ë²”ìœ„=({lon_min}, {lon_max})")

        lat_grid = np.arange(round(lat_min, 2), round(lat_max, 2) + 0.01, 0.01)
        lon_grid = np.arange(round(lon_min, 2), round(lon_max, 2) + 0.01, 0.01)

        print(f"ğŸŒ lat_grid={lat_grid.shape}, lon_grid={lon_grid.shape}")
        print(f"===== ì²˜ë¦¬ ì™„ë£Œ: {input_filename} =====\n")



        # ====== NetCDF íŒŒì¼ ê²½ë¡œ ë¯¸ë¦¬ ì„¤ì • ======
        input_basename = os.path.splitext(os.path.basename(input_file))[0]
        nc_folder = r".\KHOA_nc_data"
        os.makedirs(nc_folder, exist_ok=True)


            # ====== API í˜¸ì¶œ ë° ë³´ê°„ ìˆ˜í–‰ ======
        service_key = 'CDXK66UUPZmXiNtOX7UYBQ=='
        base_url = "http://www.khoa.go.kr/api/oceangrid/tidalCurrentAreaGeoJson/search.do"
        all_data = []


        output_path = os.path.join(nc_folder, f"{input_basename}_uv.nc")

        # ====== íŒŒì¼ ì¡´ì¬ ì‹œ ìƒëµ ======
        if os.path.exists(output_path):
            print(f"ğŸ”„ ì´ë¯¸ NetCDF ì¡´ì¬, ë‹¤ìš´ë¡œë“œ ìƒëµ: {output_path}")
        else:

        # ğŸ“Œ 5. API í˜¸ì¶œ ë° ë°ì´í„° ì €ì¥
            for t in time_list:
                params = {
                    "DataType": "tidalCurrentAreaGeoJson",
                    "ServiceKey": service_key,
                    "Date": t.strftime("%Y%m%d"),
                    "Hour": t.strftime("%H"),
                    "Minute": "00",
                    "MinX": lon_min,
                    "MaxX": lon_max,
                    "MinY": lat_min,
                    "MaxY": lat_max,
                    "Scale": 2000000
                }

                try:
                    response = requests.get(base_url, params=params)
                    if response.status_code == 200:
                        geojson_data = response.json()
                        print(geojson_data)
                        for feature in geojson_data.get("features", []):
                            p = feature["properties"]
                            lat = p.get("lat")
                            lon = p.get("lon")
                            spd = p.get("current_speed")
                            direction = p.get("current_direct")
                            if None in (lat, lon, spd, direction):
                                continue
                            spd_m = spd / 100
                            rad = np.radians(direction)
                            u = spd_m * np.sin(rad)
                            v = spd_m * np.cos(rad)
                            all_data.append({
                                "time": t,
                                "lat": lat,
                                "lon": lon,
                                "u": u,
                                "v": v
                            })

                        else:
                            # ğŸ’¡ ì‘ë‹µì´ 200ì´ì§€ë§Œ GeoJSONì´ ì•„ë‹ ë•Œ ë³¸ë¬¸ì„ ì¶œë ¥í•˜ì—¬ ì›ì¸ íŒŒì•…
                            print(f"âŒ API ì‹¤íŒ¨: status=200. ì‘ë‹µ ë³¸ë¬¸ì´ GeoJSONì´ ì•„ë‹˜.")
                            print(f"   (ì‹œê°„: {t.strftime('%Y%m%d %H:%M')}) ì‘ë‹µ: {response.text[:100]}...") # ì²˜ìŒ 100ì ì¶œë ¥
                    else:
                        print(f"âŒ API ì‹¤íŒ¨: status={response.status_code}")
                except Exception as e:
                    print(f"[ì˜ˆì™¸] {e}")

            # ğŸ“Œ 6. ì •ë°©ê²©ì ë³´ê°„ ë° NetCDF ìƒì„±
            df_all = pd.DataFrame(all_data)
            times = sorted(df_all["time"].unique())
            u_interp = []
            v_interp = []

            lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)

            for t in times:
                sub = df_all[df_all["time"] == t]
                points = np.array(sub[["lon", "lat"]])
                u_vals = sub["u"].values
                v_vals = sub["v"].values

                u_grid = griddata(points, u_vals, (lon_mesh, lat_mesh), method='linear')
                v_grid = griddata(points, v_vals, (lon_mesh, lat_mesh), method='linear')

                u_interp.append(u_grid)
                v_interp.append(v_grid)

            # 7. OpenDrift ì¸ì‹ ê°€ëŠ¥í•˜ë„ë¡ ë³€ìˆ˜ëª… + ë©”íƒ€ë°ì´í„° ì„¤ì •
            ds = xr.Dataset(
                {
                    "eastward_sea_water_velocity": (["time", "lat", "lon"], np.array(u_interp)),
                    "northward_sea_water_velocity": (["time", "lat", "lon"], np.array(v_interp)),
                },
                coords={
                    "time": times,
                    "lat": lat_grid,
                    "lon": lon_grid,
                },
                attrs={
                    "title": "ì •ë°©ê²©ì ë³´ê°„ëœ KHOA í•´ë¥˜ ì˜ˆì¸¡ ë°ì´í„°",
                    "source": "tidalCurrentAreaGeoJson API"
                }
            )

            # ë³€ìˆ˜ì— CF-convention ë©”íƒ€ë°ì´í„° ì¶”ê°€
            ds["eastward_sea_water_velocity"].attrs["standard_name"] = "eastward_sea_water_velocity"
            ds["eastward_sea_water_velocity"].attrs["units"] = "m s-1"
            ds["northward_sea_water_velocity"].attrs["standard_name"] = "northward_sea_water_velocity"
            ds["northward_sea_water_velocity"].attrs["units"] = "m s-1"

            nc_folder = r".\KHOA_nc_data"
            os.makedirs(nc_folder, exist_ok=True)

            output_path = os.path.join(nc_folder, f"{input_basename}_uv.nc")
            ds.to_netcdf(output_path)
            print(f"âœ… NetCDF ì €ì¥ ì™„ë£Œ: {output_path}")



        #=======================================================
        # PART 3: ERA5 CDS API ë°ì´í„° ë‹¤ìš´ë¡œë“œã…¡
        #=======================================================

        # ë‚ ì§œ ë²”ìœ„ ìë™ ìƒì„± (ì˜ˆ: ['09','10','11'] ë“±)
        num_days = (end_time.date() - start_time.date()).days + 1
        days = [(start_time + timedelta(days=i)).strftime("%d") for i in range(num_days)]

        # ì‹œê°„ ë¦¬ìŠ¤íŠ¸ (00:00 ~ 23:00)
        times = [f"{h:02d}:00" for h in range(24)]


        area   = [lat_max, lon_min, lat_min, lon_max]

        era5_request = {
            "product_type": ["reanalysis"],
            "variable": [
                "10m_u_component_of_wind",
                "10m_v_component_of_wind"
            ],
            "year":  [year],
            "month": [month],
            "day":   days,
            "time":  times,
            "area":  area,
            "format": "netcdf"
        }


        # ====== ì €ì¥ ê²½ë¡œ ë° íŒŒì¼ëª… =====
        print("ERA5 ìš”ì²­:", era5_request)
        client = cdsapi.Client()
        wind_folder = r".\wind_data"
        os.makedirs(wind_folder, exist_ok=True)
        wind_path = os.path.join(wind_folder, f"{input_basename}_wind.nc")

        # ====== íŒŒì¼ ì¡´ì¬ ì‹œ ë‹¤ìš´ë¡œë“œ ìƒëµ ======
        if os.path.exists(wind_path):
            print(f"ğŸ”„ ì´ë¯¸ wind íŒŒì¼ ì¡´ì¬, ë‹¤ìš´ë¡œë“œ ìƒëµ: {wind_path}")
        else:
            print("ğŸŒ¬ï¸ ERA5 wind ìš”ì²­:", era5_request)
            client = cdsapi.Client()
            client.retrieve(
                'reanalysis-era5-single-levels',
                era5_request,
                wind_path
            )
            print(f"âœ… ERA5 ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {wind_path}")


        ###############################################################################
        # ê°€ì‹œê±°ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        ###############################################################################

        # ê´€ì¸¡ì†Œ ëª©ë¡ (ìœ„ë„, ê²½ë„)
        observation_stations = {
            "SF_0001": {"name": "ë¶€ì‚°í•­", "latitude": 35.091, "longitude": 129.099},
            "SF_0002": {"name": "ë¶€ì‚°í•­(ì‹ í•­)", "latitude": 35.023, "longitude": 128.808},
            "SF_0009": {"name": "í•´ìš´ëŒ€", "latitude": 35.15909, "longitude": 129.16026},
            "SF_0010": {"name": "ìš¸ì‚°í•­", "latitude": 35.501, "longitude": 129.387},
            "SF_0008": {"name": "ì—¬ìˆ˜í•­", "latitude": 34.754, "longitude": 127.752},
        }

        # API í‚¤
        service_key = 'ANM8LV6zTsRNiGg6FCUMpw=='  # ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤

        # JSON íŒŒì¼ ë¡œë“œ
        json_file = input_file

        # fishery_behaviorê°€ 1ì¸ ë°ì´í„° ì¶”ì¶œ (ì²« ë²ˆì§¸ë§Œ)
        first_fishery_behavior = None
        for feature in data['features']:
            if feature['properties']['fishery_behavior'] == 1:
                first_fishery_behavior = feature
                break  # ì²« ë²ˆì§¸ ë°ì´í„°ë§Œ ì²˜ë¦¬

        # ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œ ì°¾ê¸°
        def find_closest_station(lat, lon):
            closest_station = None
            min_distance = float('inf')

            # ê° ê´€ì¸¡ì†Œì™€ì˜ ê±°ë¦¬ ê³„ì‚°
            for obs_code, station in observation_stations.items():
                station_location = (station["latitude"], station["longitude"])
                current_location = (lat, lon)
                distance = geodesic(station_location, current_location).kilometers
                
                if distance < min_distance:
                    min_distance = distance
                    closest_station = obs_code
            
            return closest_station
        

        # ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œì—ì„œ ê°€ì‹œê±°ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        def get_visibility_from_station(obs_code, timestamp):
            # ë‚ ì§œë§Œ ì¶”ì¶œí•´ì„œ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            date_only = timestamp.split(" ")[0].replace("-", "")  # ë‚ ì§œë§Œ ì¶”ì¶œ (YYYYMMDD)

            # API ìš”ì²­ URL ìƒì„±
            url = f"http://www.khoa.go.kr/api/oceangrid/seafogReal/search.do" \
                f"?DataType=seafogReal" \
                f"&ServiceKey={service_key}" \
                f"&ObsCode={obs_code}" \
                f"&Date={date_only}" \
                f"&ResultType=json"
            
            # API ìš”ì²­
            response = requests.get(url)

            # ì‘ë‹µ ë°ì´í„° í™•ì¸
            if response.status_code == 200:
                data = response.json()
                
                # ì‘ë‹µ ë°ì´í„° ì¶œë ¥
                if 'result' in data and 'data' in data['result']:
                    closest_time_diff = float('inf')  # ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ ì°¨ì´
                    closest_visibility = None

                    for observation in data['result']['data']:
                        obs_time = observation['obs_time']

                        # ì‹œê°„ ì°¨ì´ ê³„ì‚° (ë‘ ì‹œê°„ì˜ ì°¨ì´ë¥¼ ë¶„ ë‹¨ìœ„ë¡œ ê³„ì‚°)
                        try:
                            timestamp_dt = parser.parse(timestamp.strip())
                            obs_time_dt = parser.parse(obs_time.strip())
                        except Exception as e:
                            print(f"ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: {e}")
                            continue

                        time_diff = abs((timestamp_dt - obs_time_dt).total_seconds())  # ì‹œê°„ ì°¨ì´ (ì´ˆ ë‹¨ìœ„)

                        # ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ ì°¾ê¸°
                        if time_diff < closest_time_diff:
                            closest_time_diff = time_diff
                            if 'vis' in observation:
                                closest_visibility = observation['vis']
                    
                    if closest_visibility:
                        return closest_visibility  # ê°€ì¥ ê°€ê¹Œìš´ ê°€ì‹œê±°ë¦¬ ë°˜í™˜
            return None

        # ì´ˆê¸° ë³€ìˆ˜
        visibility = None
        latitude = None
        longitude = None
        timestamp = None
        closest_station = None

        # fishery_behavior = 1ì¸ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì •ë³´ ì €ì¥
        if first_fishery_behavior:
            timestamp = first_fishery_behavior['properties']['time_stamp']
            latitude = first_fishery_behavior['properties']['latitude']
            longitude = first_fishery_behavior['properties']['longitude']
            closest_station = find_closest_station(latitude, longitude)
        else:
            print("fishery_behaviorê°€ 1ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # CSV ê²½ë¡œ ì§€ì •
        output_csv_path = r".\ê°€ì‹œê±°ë¦¬csv\visibility_log_train3.csv"

        # 1. CSVì—ì„œ í™•ì¸
        if os.path.exists(output_csv_path):
            with open(output_csv_path, mode='r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if row["filename"] == input_filename:
                        visibility = row["visibility_m"]
                        print(f"ğŸ”„ ê¸°ì¡´ CSVì—ì„œ ê°€ì‹œê±°ë¦¬ ë¶ˆëŸ¬ì˜´: {visibility}")
                        break

        # 2. ì—†ìœ¼ë©´ API í˜¸ì¶œ
        if visibility is None  and timestamp and closest_station:
            visibility = get_visibility_from_station(closest_station, timestamp)

            if visibility:
                print(f"ì‹œê°„: {timestamp} / ìœ„ì¹˜: ({latitude}, {longitude})")
                print(f"ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œ: {observation_stations[closest_station]['name']} ({closest_station})")
                print(f"ê°€ì‹œê±°ë¦¬: {visibility} ë¯¸í„°")
            else:
                print(f"ê°€ì‹œê±°ë¦¬ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 3. CSVì— ì €ì¥
        if not os.path.exists(output_csv_path):
            with open(output_csv_path, mode='w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow(["filename", "visibility_m"])

        # 4. ì¤‘ë³µ ì €ì¥ ë°©ì§€ í›„ ì¶”ê°€
        already_exists = False
        with open(output_csv_path, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row["filename"] == input_filename:
                    already_exists = True
                    break

        if not already_exists:
            with open(output_csv_path, mode='a', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerow([input_filename, visibility if visibility else "N/A"])



        ###############################################################################
        # (4) ERA5 windíŒŒì¼ ê²½ë¡œ
        ###############################################################################
        nc_folder = r".\KHOA_nc_data"
        merged_file = os.path.join(nc_folder, f"{input_basename}_uv.nc")

        hycom_folder = r".\hycom_data"
        hycom_file = os.path.join(hycom_folder, f"{input_basename}_hycom.nc")

        wind_file = os.path.join(wind_folder, f"{input_basename}_wind.nc")


        # bottom_depth = r"C:\Users\HUFS\Desktop\opendrift_middle\bottom_depth.nc"
        # print("Bottom depth file:", bottom_depth)





        ###############################################################################
        # PART 3: í•´ì•ˆì„  ì½ê¸°
        ###############################################################################
        # í•´ì•ˆì„  ì½ê¸°
        coastline_file = r"C:\Users\HUFS\Downloads\í•´ì–‘ìˆ˜ì‚°ë¶€ êµ­ë¦½í•´ì–‘ì¡°ì‚¬ì›_í•´ì•ˆì„ _20241231\2025ë…„ ì „êµ­ í•´ì•ˆì„ .shp"
        coast = gpd.read_file(coastline_file)
        if coast.crs is None or coast.crs.to_string() != 'EPSG:4326':
            coast = coast.to_crs(epsg=4326)
        coast_proj = coast.to_crs(epsg=3857)
        coastal_zone = coast_proj.buffer(15000).union_all()
        coastal_zone_wgs84 = gpd.GeoSeries(coastal_zone, crs=3857).to_crs(epsg=4326).union_all()
        print("í•´ì•ˆì„  ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")



    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {input_file} - {e}")
        with open(error_log_path, "a", newline="", encoding="utf-8-sig") as f:
            writer = csv.writer(f)
            writer.writerow([os.path.basename(input_file), type(e).__name__, str(e)])
        continue